{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Houses dataset"
      ],
      "metadata": {
        "id": "MRTSw7qhGkWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/emanhamed/Houses-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJ-LsV09c7m_",
        "outputId": "90f0c6ed-6714-4810-e04d-360463d98db9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Houses-dataset'...\n",
            "remote: Enumerating objects: 2166, done.\u001b[K\n",
            "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 2166 (delta 0), reused 0 (delta 0), pack-reused 2165\u001b[K\n",
            "Receiving objects: 100% (2166/2166), 176.26 MiB | 31.64 MiB/s, done.\n",
            "Resolving deltas: 100% (20/20), done.\n",
            "Updating files: 100% (2144/2144), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# where is HousesInfo.txt ???\n",
        "import os\n",
        "\n",
        "for root, dirs, files in os.walk(\"/content/Houses-dataset/Houses Dataset\"):\n",
        "  for f in files:\n",
        "    if f.split('.')[-1] == 'txt':\n",
        "      print(os.path.join(root, f))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqbCoPMzc_S2",
        "outputId": "37775c7f-c1b4-4b79-b486-9cf26dc301bd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Houses-dataset/Houses Dataset/HousesInfo.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import the necessary packages\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "\n",
        "def load_house_attributes(inputPath):\n",
        "\t# initialize the list of column names in the CSV file and then load it using Pandas\n",
        "\tcols = [\"bedrooms\", \"bathrooms\", \"area\", \"zipcode\", \"price\"]\n",
        "\tdf = pd.read_csv(inputPath, sep=\" \", header=None, names=cols)\n",
        "\n",
        "\t# determine (1) the unique zip codes and (2) the number of data points with each zip code\n",
        "\tzipcodes = df[\"zipcode\"].value_counts().keys().tolist()\n",
        "\tcounts = df[\"zipcode\"].value_counts().tolist()\n",
        "\n",
        "\t# loop over each of the unique zip codes and their corresponding count\n",
        "\tfor (zipcode, count) in zip(zipcodes, counts):\n",
        "\t\t# the zip code counts for our housing dataset is *extremely*\n",
        "\t\t# unbalanced (some only having 1 or 2 houses per zip code)\n",
        "\t\t# so let's sanitize our data by removing any houses with less\n",
        "\t\t# than 25 houses per zip code\n",
        "\t\tif count < 25:\n",
        "\t\t\tidxs = df[df[\"zipcode\"] == zipcode].index\n",
        "\t\t\tdf.drop(idxs, inplace=True)\n",
        "\n",
        "\t# return the data frame\n",
        "\treturn df"
      ],
      "metadata": {
        "id": "7p22I7iVfJBA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputPath = \"/content/Houses-dataset/Houses Dataset/HousesInfo.txt\"\n",
        "\n",
        "load_house_attributes(inputPath)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "wQ_H5z5sg_FC",
        "outputId": "a3378d0e-31a3-4967-e53e-016df0329592"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     bedrooms  bathrooms  area  zipcode    price\n",
              "30          5        3.0  2520    93446   789000\n",
              "32          3        2.0  1802    93446   365000\n",
              "39          3        3.0  2146    93446   455000\n",
              "80          4        2.5  2464    91901   599000\n",
              "81          2        2.0  1845    91901   529800\n",
              "..        ...        ...   ...      ...      ...\n",
              "499         4        4.0  3000    93446  1495000\n",
              "500         3        2.0  2330    93446   599900\n",
              "501         3        2.5  1339    93446   344900\n",
              "502         3        2.0  1472    93446   309995\n",
              "503         4        4.0  2681    93446   572000\n",
              "\n",
              "[362 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-358460a6-a893-44ad-93ce-71539a2f35d8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>area</th>\n",
              "      <th>zipcode</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2520</td>\n",
              "      <td>93446</td>\n",
              "      <td>789000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>3</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1802</td>\n",
              "      <td>93446</td>\n",
              "      <td>365000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2146</td>\n",
              "      <td>93446</td>\n",
              "      <td>455000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>4</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2464</td>\n",
              "      <td>91901</td>\n",
              "      <td>599000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1845</td>\n",
              "      <td>91901</td>\n",
              "      <td>529800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>4</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3000</td>\n",
              "      <td>93446</td>\n",
              "      <td>1495000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>500</th>\n",
              "      <td>3</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2330</td>\n",
              "      <td>93446</td>\n",
              "      <td>599900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>501</th>\n",
              "      <td>3</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1339</td>\n",
              "      <td>93446</td>\n",
              "      <td>344900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>502</th>\n",
              "      <td>3</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1472</td>\n",
              "      <td>93446</td>\n",
              "      <td>309995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>503</th>\n",
              "      <td>4</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2681</td>\n",
              "      <td>93446</td>\n",
              "      <td>572000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>362 rows Ã— 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-358460a6-a893-44ad-93ce-71539a2f35d8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-d32533c8-866b-4875-a514-1f4351623762\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d32533c8-866b-4875-a514-1f4351623762')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-d32533c8-866b-4875-a514-1f4351623762 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-358460a6-a893-44ad-93ce-71539a2f35d8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-358460a6-a893-44ad-93ce-71539a2f35d8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_house_attributes(df, train, test):\n",
        "  # initialize the column names of the continuous data\n",
        "  continuous = [\"bedrooms\", \"bathrooms\", \"area\"]\n",
        "\n",
        "  # perform min-max scaling each continuous feature column to the range [0, 1]\n",
        "  cs = MinMaxScaler()\n",
        "  trainContinuous = cs.fit_transform(train[continuous])\n",
        "  testContinuous = cs.transform(test[continuous])\n",
        "\n",
        "  # one-hot encode the zip code categorical data (by definition of\n",
        "  # one-hot encoing, all output features are now in the range [0, 1])\n",
        "  zipBinarizer = LabelBinarizer().fit(df[\"zipcode\"])\n",
        "  trainCategorical = zipBinarizer.transform(train[\"zipcode\"])\n",
        "  testCategorical = zipBinarizer.transform(test[\"zipcode\"])\n",
        "\n",
        "  # construct our training and testing data points by concatenating\n",
        "  # the categorical features with the continuous features\n",
        "  trainX = np.hstack([trainCategorical, trainContinuous])\n",
        "  testX = np.hstack([testCategorical, testContinuous])\n",
        "\n",
        "  # return the concatenated training and testing data\n",
        "  return (trainX, testX)"
      ],
      "metadata": {
        "id": "4gVwAXzbUz76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Activation, Dropout, Dense, Flatten, Input\n",
        "\n",
        "\n",
        "def create_mlp(dim, regress=False):\n",
        "  # define our MLP network\n",
        "  model = Sequential()\n",
        "  model.add(Dense(512, input_dim=dim, activation=\"relu\"))\n",
        "  model.add(Dense(256, activation=\"relu\"))\n",
        "  model.add(Dense(64, activation=\"relu\"))\n",
        "  model.add(Dense(4, activation=\"relu\"))\n",
        "\n",
        "  # check to see if the regression node should be added\n",
        "  if regress:\n",
        "    model.add(Dense(1, activation=\"linear\"))\n",
        "  # return our model\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "Dl7PHC9HWSEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import argparse\n",
        "import locale\n",
        "import os\n",
        "\n",
        "inputPath = \"/content/Houses-dataset/Houses Dataset/HousesInfo.txt\"\n",
        "\n",
        "df = load_house_attributes(inputPath)\n",
        "# construct a training and testing split with 75% of the data used\n",
        "# for training and the remaining 25% for evaluation\n",
        "(train, test) = train_test_split(df, test_size=0.25, random_state=42)\n",
        "\n",
        "maxPrice = train[\"price\"].max()\n",
        "trainY = train[\"price\"] / maxPrice\n",
        "testY = test[\"price\"] / maxPrice\n",
        "\n",
        "# process the house attributes data by performing min-max scaling\n",
        "# on continuous features, one-hot encoding on categorical features,\n",
        "# and then finally concatenating them together\n",
        "# [INFO] processing data\n",
        "(trainX, testX) = process_house_attributes(df, train, test)\n",
        "\n",
        "\n",
        "# create our MLP and then compile the model using mean absolute\n",
        "# percentage error as our loss, implying that we seek to minimize\n",
        "# the absolute percentage difference between our price *predictions*\n",
        "# and the *actual prices*\n",
        "model = create_mlp(trainX.shape[1], regress=True)\n",
        "\n",
        "model.compile(loss=\"mean_absolute_percentage_error\", optimizer='Adam')\n",
        "\n",
        "# train the model\n",
        "print(\"[INFO] training model...\")\n",
        "model.fit(x=trainX, y=trainY,\n",
        "\tvalidation_data=(testX, testY),\n",
        "\tepochs=200, batch_size=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b0Uu2CRYueF",
        "outputId": "314848c8-32fb-4a79-93ed-d2580e0d1e89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] training model...\n",
            "Epoch 1/200\n",
            "34/34 [==============================] - 2s 14ms/step - loss: 50.0521 - val_loss: 44.7075\n",
            "Epoch 2/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 36.5598 - val_loss: 30.3653\n",
            "Epoch 3/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 29.8336 - val_loss: 27.5729\n",
            "Epoch 4/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 28.0566 - val_loss: 26.8745\n",
            "Epoch 5/200\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 26.7702 - val_loss: 21.7125\n",
            "Epoch 6/200\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 26.5367 - val_loss: 25.6252\n",
            "Epoch 7/200\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 25.3498 - val_loss: 30.1466\n",
            "Epoch 8/200\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 27.1091 - val_loss: 34.1324\n",
            "Epoch 9/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 30.1158 - val_loss: 25.3826\n",
            "Epoch 10/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 25.8908 - val_loss: 30.9201\n",
            "Epoch 11/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 27.2226 - val_loss: 26.7833\n",
            "Epoch 12/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 24.9130 - val_loss: 22.6121\n",
            "Epoch 13/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 26.6686 - val_loss: 23.5693\n",
            "Epoch 14/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 23.9890 - val_loss: 26.5897\n",
            "Epoch 15/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 25.6544 - val_loss: 25.2141\n",
            "Epoch 16/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 24.8152 - val_loss: 20.9408\n",
            "Epoch 17/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 25.5168 - val_loss: 24.4307\n",
            "Epoch 18/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 25.0092 - val_loss: 23.6964\n",
            "Epoch 19/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 23.7513 - val_loss: 23.0893\n",
            "Epoch 20/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 23.5358 - val_loss: 25.2408\n",
            "Epoch 21/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 23.9825 - val_loss: 22.2769\n",
            "Epoch 22/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 23.2113 - val_loss: 22.8437\n",
            "Epoch 23/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 25.3194 - val_loss: 25.7185\n",
            "Epoch 24/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 24.2830 - val_loss: 20.9720\n",
            "Epoch 25/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 22.6865 - val_loss: 24.2067\n",
            "Epoch 26/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 22.8479 - val_loss: 23.7254\n",
            "Epoch 27/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 22.6922 - val_loss: 21.5207\n",
            "Epoch 28/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 24.2484 - val_loss: 21.9463\n",
            "Epoch 29/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 22.2762 - val_loss: 22.7846\n",
            "Epoch 30/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 23.4745 - val_loss: 26.4092\n",
            "Epoch 31/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 24.0237 - val_loss: 26.0351\n",
            "Epoch 32/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 24.2598 - val_loss: 23.3903\n",
            "Epoch 33/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 23.6471 - val_loss: 21.8502\n",
            "Epoch 34/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 22.5003 - val_loss: 21.7285\n",
            "Epoch 35/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 22.4679 - val_loss: 22.7381\n",
            "Epoch 36/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 24.6265 - val_loss: 22.0156\n",
            "Epoch 37/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 22.9048 - val_loss: 27.0504\n",
            "Epoch 38/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 22.0178 - val_loss: 20.7967\n",
            "Epoch 39/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 22.0706 - val_loss: 24.7057\n",
            "Epoch 40/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 22.1290 - val_loss: 21.9436\n",
            "Epoch 41/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 24.3606 - val_loss: 22.0590\n",
            "Epoch 42/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 21.0800 - val_loss: 22.7644\n",
            "Epoch 43/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 21.2914 - val_loss: 19.8599\n",
            "Epoch 44/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 20.0396 - val_loss: 17.9523\n",
            "Epoch 45/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 22.8236 - val_loss: 20.7521\n",
            "Epoch 46/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 21.6005 - val_loss: 22.1627\n",
            "Epoch 47/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 21.3123 - val_loss: 21.8514\n",
            "Epoch 48/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 21.3331 - val_loss: 22.0730\n",
            "Epoch 49/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 20.6015 - val_loss: 23.2340\n",
            "Epoch 50/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 20.7117 - val_loss: 24.1842\n",
            "Epoch 51/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 23.0806 - val_loss: 21.3192\n",
            "Epoch 52/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 20.6428 - val_loss: 21.4956\n",
            "Epoch 53/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 21.2032 - val_loss: 24.6335\n",
            "Epoch 54/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 20.3454 - val_loss: 26.3801\n",
            "Epoch 55/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 21.4093 - val_loss: 19.8817\n",
            "Epoch 56/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 20.1678 - val_loss: 20.8313\n",
            "Epoch 57/200\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 19.7076 - val_loss: 20.8142\n",
            "Epoch 58/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 19.8697 - val_loss: 19.1710\n",
            "Epoch 59/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 20.0921 - val_loss: 23.8174\n",
            "Epoch 60/200\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 19.3536 - val_loss: 23.7665\n",
            "Epoch 61/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 21.1145 - val_loss: 21.7641\n",
            "Epoch 62/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 19.3183 - val_loss: 22.5464\n",
            "Epoch 63/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 20.4539 - val_loss: 23.8320\n",
            "Epoch 64/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 20.4627 - val_loss: 22.6113\n",
            "Epoch 65/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 20.1075 - val_loss: 22.2004\n",
            "Epoch 66/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 20.1022 - val_loss: 22.0762\n",
            "Epoch 67/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 20.2288 - val_loss: 23.5620\n",
            "Epoch 68/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 20.6137 - val_loss: 23.9268\n",
            "Epoch 69/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 20.5003 - val_loss: 22.5860\n",
            "Epoch 70/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 20.3491 - val_loss: 23.3089\n",
            "Epoch 71/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 19.5946 - val_loss: 23.2992\n",
            "Epoch 72/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 20.0597 - val_loss: 21.6253\n",
            "Epoch 73/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 21.1871 - val_loss: 21.2526\n",
            "Epoch 74/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 18.9513 - val_loss: 20.4407\n",
            "Epoch 75/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 18.3915 - val_loss: 20.9196\n",
            "Epoch 76/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 19.7704 - val_loss: 23.4062\n",
            "Epoch 77/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 19.5803 - val_loss: 19.6574\n",
            "Epoch 78/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 18.7093 - val_loss: 23.0676\n",
            "Epoch 79/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 19.1106 - val_loss: 20.3432\n",
            "Epoch 80/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 18.0158 - val_loss: 20.1214\n",
            "Epoch 81/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 21.2743 - val_loss: 21.3555\n",
            "Epoch 82/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 20.8907 - val_loss: 23.2458\n",
            "Epoch 83/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 20.7519 - val_loss: 22.9822\n",
            "Epoch 84/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 20.7537 - val_loss: 22.1441\n",
            "Epoch 85/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 19.9915 - val_loss: 22.9799\n",
            "Epoch 86/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 20.3716 - val_loss: 22.3796\n",
            "Epoch 87/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 19.4079 - val_loss: 21.9397\n",
            "Epoch 88/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 19.3387 - val_loss: 19.5060\n",
            "Epoch 89/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 20.3394 - val_loss: 21.9433\n",
            "Epoch 90/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 19.0742 - val_loss: 20.5556\n",
            "Epoch 91/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 18.7834 - val_loss: 21.5498\n",
            "Epoch 92/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 18.2731 - val_loss: 18.9435\n",
            "Epoch 93/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 19.0679 - val_loss: 24.4031\n",
            "Epoch 94/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 21.1557 - val_loss: 23.0402\n",
            "Epoch 95/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 19.6044 - val_loss: 22.3984\n",
            "Epoch 96/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 18.5440 - val_loss: 20.6174\n",
            "Epoch 97/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 20.5284 - val_loss: 21.1000\n",
            "Epoch 98/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 18.9142 - val_loss: 21.7980\n",
            "Epoch 99/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 17.4507 - val_loss: 19.4510\n",
            "Epoch 100/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 18.6226 - val_loss: 20.4347\n",
            "Epoch 101/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 18.7449 - val_loss: 21.7568\n",
            "Epoch 102/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 18.0185 - val_loss: 21.9341\n",
            "Epoch 103/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 17.7353 - val_loss: 18.7365\n",
            "Epoch 104/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 18.3363 - val_loss: 19.7407\n",
            "Epoch 105/200\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 20.1057 - val_loss: 20.2677\n",
            "Epoch 106/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 19.1395 - val_loss: 21.1867\n",
            "Epoch 107/200\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 18.0119 - val_loss: 21.9913\n",
            "Epoch 108/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 17.3723 - val_loss: 20.8497\n",
            "Epoch 109/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 17.8171 - val_loss: 21.8595\n",
            "Epoch 110/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 18.1974 - val_loss: 20.2503\n",
            "Epoch 111/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 18.2321 - val_loss: 24.2696\n",
            "Epoch 112/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 19.0218 - val_loss: 19.2370\n",
            "Epoch 113/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 17.4348 - val_loss: 21.9407\n",
            "Epoch 114/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 17.8393 - val_loss: 19.6929\n",
            "Epoch 115/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 17.9899 - val_loss: 22.1105\n",
            "Epoch 116/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 18.7795 - val_loss: 22.9929\n",
            "Epoch 117/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 18.3200 - val_loss: 22.5659\n",
            "Epoch 118/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 17.9774 - val_loss: 19.8642\n",
            "Epoch 119/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 17.3786 - val_loss: 21.0380\n",
            "Epoch 120/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 18.5731 - val_loss: 22.6375\n",
            "Epoch 121/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 17.8760 - val_loss: 21.3968\n",
            "Epoch 122/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 17.4312 - val_loss: 21.2833\n",
            "Epoch 123/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 17.8002 - val_loss: 20.5496\n",
            "Epoch 124/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 18.3540 - val_loss: 20.9346\n",
            "Epoch 125/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 17.2080 - val_loss: 22.3645\n",
            "Epoch 126/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 18.1150 - val_loss: 20.4574\n",
            "Epoch 127/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 17.1198 - val_loss: 19.7984\n",
            "Epoch 128/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 16.8779 - val_loss: 21.3914\n",
            "Epoch 129/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 16.9656 - val_loss: 19.8725\n",
            "Epoch 130/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 18.7347 - val_loss: 21.9005\n",
            "Epoch 131/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 17.2727 - val_loss: 20.4149\n",
            "Epoch 132/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 17.7502 - val_loss: 21.0205\n",
            "Epoch 133/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 16.5930 - val_loss: 23.0970\n",
            "Epoch 134/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 18.0223 - val_loss: 22.8832\n",
            "Epoch 135/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 17.6380 - val_loss: 20.7561\n",
            "Epoch 136/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 17.5041 - val_loss: 20.4252\n",
            "Epoch 137/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 17.1545 - val_loss: 20.4622\n",
            "Epoch 138/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 16.7459 - val_loss: 22.6600\n",
            "Epoch 139/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 18.0740 - val_loss: 23.9472\n",
            "Epoch 140/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 17.5627 - val_loss: 22.8390\n",
            "Epoch 141/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 18.3129 - val_loss: 20.5487\n",
            "Epoch 142/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 17.0076 - val_loss: 20.4593\n",
            "Epoch 143/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 17.5181 - val_loss: 20.6731\n",
            "Epoch 144/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 16.8959 - val_loss: 20.6815\n",
            "Epoch 145/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 17.0309 - val_loss: 20.1839\n",
            "Epoch 146/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 16.3653 - val_loss: 21.3080\n",
            "Epoch 147/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 16.4439 - val_loss: 22.0891\n",
            "Epoch 148/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 17.1880 - val_loss: 22.7074\n",
            "Epoch 149/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 16.7136 - val_loss: 19.2255\n",
            "Epoch 150/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 18.4594 - val_loss: 21.6460\n",
            "Epoch 151/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 16.3892 - val_loss: 20.2712\n",
            "Epoch 152/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 16.7775 - val_loss: 20.9855\n",
            "Epoch 153/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 18.6934 - val_loss: 21.8454\n",
            "Epoch 154/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 17.8982 - val_loss: 22.2994\n",
            "Epoch 155/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 16.5572 - val_loss: 20.1852\n",
            "Epoch 156/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 16.9509 - val_loss: 20.7389\n",
            "Epoch 157/200\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 16.5715 - val_loss: 20.8688\n",
            "Epoch 158/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 17.1186 - val_loss: 22.3981\n",
            "Epoch 159/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 16.8956 - val_loss: 23.1896\n",
            "Epoch 160/200\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 17.1011 - val_loss: 20.4293\n",
            "Epoch 161/200\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 16.7355 - val_loss: 21.2924\n",
            "Epoch 162/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 17.0733 - val_loss: 21.3665\n",
            "Epoch 163/200\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 16.1918 - val_loss: 20.7349\n",
            "Epoch 164/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 16.7979 - val_loss: 21.0382\n",
            "Epoch 165/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 18.4330 - val_loss: 25.8460\n",
            "Epoch 166/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 16.9189 - val_loss: 20.9955\n",
            "Epoch 167/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 16.3444 - val_loss: 23.3131\n",
            "Epoch 168/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 17.2351 - val_loss: 19.6156\n",
            "Epoch 169/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 16.4508 - val_loss: 22.9912\n",
            "Epoch 170/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 17.4991 - val_loss: 19.2767\n",
            "Epoch 171/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 17.7619 - val_loss: 22.4118\n",
            "Epoch 172/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 17.9084 - val_loss: 19.6163\n",
            "Epoch 173/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 16.5843 - val_loss: 21.7078\n",
            "Epoch 174/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 17.3639 - val_loss: 20.5254\n",
            "Epoch 175/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 16.7774 - val_loss: 22.3191\n",
            "Epoch 176/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 16.3516 - val_loss: 19.5985\n",
            "Epoch 177/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 16.2522 - val_loss: 23.3978\n",
            "Epoch 178/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 17.0648 - val_loss: 21.2098\n",
            "Epoch 179/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 17.1177 - val_loss: 20.6873\n",
            "Epoch 180/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 17.3930 - val_loss: 19.9058\n",
            "Epoch 181/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 17.0827 - val_loss: 20.7845\n",
            "Epoch 182/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 16.0514 - val_loss: 22.6070\n",
            "Epoch 183/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 17.5075 - val_loss: 21.4396\n",
            "Epoch 184/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 16.4891 - val_loss: 20.0018\n",
            "Epoch 185/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 15.7659 - val_loss: 20.9531\n",
            "Epoch 186/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 16.3227 - val_loss: 21.1632\n",
            "Epoch 187/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 16.4346 - val_loss: 22.3043\n",
            "Epoch 188/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 15.5311 - val_loss: 20.8269\n",
            "Epoch 189/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 17.4078 - val_loss: 22.0182\n",
            "Epoch 190/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 17.8336 - val_loss: 21.9041\n",
            "Epoch 191/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 16.5070 - val_loss: 20.6871\n",
            "Epoch 192/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 16.7033 - val_loss: 19.9683\n",
            "Epoch 193/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 17.5534 - val_loss: 21.2186\n",
            "Epoch 194/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 18.0108 - val_loss: 20.9000\n",
            "Epoch 195/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 16.2330 - val_loss: 22.0494\n",
            "Epoch 196/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 17.0574 - val_loss: 22.9963\n",
            "Epoch 197/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 16.0509 - val_loss: 22.2516\n",
            "Epoch 198/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 16.4486 - val_loss: 20.6023\n",
            "Epoch 199/200\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 16.7032 - val_loss: 20.7548\n",
            "Epoch 200/200\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 15.9267 - val_loss: 21.3415\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ab3ec5fe3e0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make predictions on the testing data\n",
        "print(\"[INFO] predicting house prices...\")\n",
        "preds = model.predict(testX)\n",
        "\n",
        "# compute the difference between the *predicted* house prices and the\n",
        "# *actual* house prices, then compute the percentage difference and\n",
        "# the absolute percentage difference\n",
        "diff = preds.flatten() - testY     # flatten()  ==> reshape(91,)\n",
        "percentDiff = (diff / testY) * 100\n",
        "absPercentDiff = np.abs(percentDiff)\n",
        "\n",
        "# compute the mean and standard deviation of the absolute percentage\n",
        "# difference\n",
        "mean = np.mean(absPercentDiff)\n",
        "std = np.std(absPercentDiff)\n",
        "\n",
        "# finally, show some statistics on our model\n",
        "locale.setlocale(locale.LC_ALL, \"en_US.UTF-8\")\n",
        "print(\"[INFO] avg. house price: {}, std house price: {}\".format(\n",
        "\tlocale.currency(df[\"price\"].mean(), grouping=True),\n",
        "\tlocale.currency(df[\"price\"].std(), grouping=True)))\n",
        "print(\"[INFO] mean: {:.2f}%, std: {:.2f}%\".format(mean, std))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAqUB2Rcdapo",
        "outputId": "06d5b4ff-789b-4719-db90-7c84c959adc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] predicting house prices...\n",
            "3/3 [==============================] - 0s 5ms/step\n",
            "[INFO] avg. house price: $533,388.27, std house price: $493,403.08\n",
            "[INFO] mean: 21.34%, std: 19.36%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **keras-regression-and-cnns**"
      ],
      "metadata": {
        "id": "72C6TFsQ77Gj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputPath_df = \"/content/Houses-dataset/Houses Dataset/HousesInfo.txt\"\n",
        "df = load_house_attributes(inputPath= inputPath_df)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "PJzSjFI988Ku",
        "outputId": "2595b707-104a-4767-c1a8-c0010dd1d524"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     bedrooms  bathrooms  area  zipcode    price\n",
              "30          5        3.0  2520    93446   789000\n",
              "32          3        2.0  1802    93446   365000\n",
              "39          3        3.0  2146    93446   455000\n",
              "80          4        2.5  2464    91901   599000\n",
              "81          2        2.0  1845    91901   529800\n",
              "..        ...        ...   ...      ...      ...\n",
              "499         4        4.0  3000    93446  1495000\n",
              "500         3        2.0  2330    93446   599900\n",
              "501         3        2.5  1339    93446   344900\n",
              "502         3        2.0  1472    93446   309995\n",
              "503         4        4.0  2681    93446   572000\n",
              "\n",
              "[362 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-b8bf6c25-c17c-4e2f-81ad-2dd629b1cd52\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>area</th>\n",
              "      <th>zipcode</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2520</td>\n",
              "      <td>93446</td>\n",
              "      <td>789000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>3</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1802</td>\n",
              "      <td>93446</td>\n",
              "      <td>365000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2146</td>\n",
              "      <td>93446</td>\n",
              "      <td>455000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>4</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2464</td>\n",
              "      <td>91901</td>\n",
              "      <td>599000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1845</td>\n",
              "      <td>91901</td>\n",
              "      <td>529800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>4</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3000</td>\n",
              "      <td>93446</td>\n",
              "      <td>1495000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>500</th>\n",
              "      <td>3</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2330</td>\n",
              "      <td>93446</td>\n",
              "      <td>599900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>501</th>\n",
              "      <td>3</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1339</td>\n",
              "      <td>93446</td>\n",
              "      <td>344900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>502</th>\n",
              "      <td>3</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1472</td>\n",
              "      <td>93446</td>\n",
              "      <td>309995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>503</th>\n",
              "      <td>4</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2681</td>\n",
              "      <td>93446</td>\n",
              "      <td>572000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>362 rows Ã— 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b8bf6c25-c17c-4e2f-81ad-2dd629b1cd52')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-7cb3549f-0e74-4bf6-b9a1-d42a43e9fd06\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7cb3549f-0e74-4bf6-b9a1-d42a43e9fd06')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-7cb3549f-0e74-4bf6-b9a1-d42a43e9fd06 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b8bf6c25-c17c-4e2f-81ad-2dd629b1cd52 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b8bf6c25-c17c-4e2f-81ad-2dd629b1cd52');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputPath = \"/content/Houses-dataset/Houses Dataset\"\n",
        "\n",
        "def load_house_images(df, inputPath):\n",
        "\t# initialize our images array (i.e., the house images themselves)\n",
        "\timages = []\n",
        "\n",
        "\t# loop over the indexes of the houses\n",
        "\tfor i in df.index.values:\n",
        "\t\t# find the four images for the house and sort the file paths,\n",
        "\t\t# ensuring the four are always in the *same order*\n",
        "\t\tbasePath = os.path.sep.join([inputPath, \"{}_*\".format(i + 1)])\n",
        "\t\thousePaths = sorted(list(glob.glob(basePath)))\n",
        "\n",
        "    # initialize our list of input images along with the output image\n",
        "\t\t# after *combining* the four input images\n",
        "\t\tinputImages = []\n",
        "\t\toutputImage = np.zeros((64, 64, 3), dtype=\"uint8\")\n",
        "\n",
        "\t\t# loop over the input house paths\n",
        "\t\tfor housePath in housePaths:\n",
        "\t\t\t# load the input image, resize it to be 32 32, and then\n",
        "\t\t\t# update the list of input images\n",
        "\t\t\timage = cv2.imread(housePath)\n",
        "\t\t\timage = cv2.resize(image, (32, 32))\n",
        "\t\t\tinputImages.append(image)\n",
        "\n",
        "    # tile the four input images in the output image such the first\n",
        "\t\t# image goes in the top-right corner, the second image in the\n",
        "\t\t# top-left corner, the third image in the bottom-right corner,\n",
        "\t\t# and the final image in the bottom-left corner\n",
        "\t\toutputImage[0:32, 0:32] = inputImages[0]\n",
        "\t\toutputImage[0:32, 32:64] = inputImages[1]\n",
        "\t\toutputImage[32:64, 32:64] = inputImages[2]\n",
        "\t\toutputImage[32:64, 0:32] = inputImages[3]\n",
        "\n",
        "\n",
        "\n",
        "    # another way to attach images:\n",
        "    # up = np.hstack([inputImages[0],inputImages[1]])\n",
        "    # down = np.hstack([inputImages[3],inputImages[2]])\n",
        "    # img = np.vstack([up, down])\n",
        "    # img.shape\n",
        "\n",
        "\n",
        "\n",
        "\t\t# add the tiled image to our set of images the network will be\n",
        "\t\t# trained on\n",
        "\t\timages.append(outputImage)\n",
        "\n",
        "\t# return our set of images\n",
        "\treturn np.array(images)"
      ],
      "metadata": {
        "id": "IcUBlDrv7921"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Activation, Dropout, Dense, Flatten, Input\n",
        "\n",
        "\n",
        "def create_cnn(width, height, depth, filters=(16, 32, 64), regress=False):\n",
        "  # initialize the input shape and channel dimension, assuming\n",
        "  # TensorFlow/channels-last ordering\n",
        "  inputShape = (height, width, depth)\n",
        "  chanDim = -1\n",
        "\n",
        "  # define the model input\n",
        "  inputs = Input(shape=inputShape, batch_size=8)\n",
        "\n",
        "  # loop over the number of filters\n",
        "  for (i, f) in enumerate(filters):\n",
        "\n",
        "    # if this is the first CONV layer then set the input appropriately\n",
        "    if i == 0:\n",
        "      x = inputs\n",
        "\n",
        "    # CONV => RELU => BN => POOL\n",
        "    x = Conv2D(f, (3, 3), padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization(axis=chanDim)(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "  # flatten the volume, then FC => RELU => BN => DROPOUT\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(16)(x)\n",
        "  x = Activation(\"relu\")(x)\n",
        "  x = BatchNormalization(axis=chanDim)(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "\n",
        "  # apply another FC layer, this one to match the number of nodes\n",
        "  # coming out of the MLP\n",
        "  x = Dense(4)(x)\n",
        "  x = Activation(\"relu\")(x)\n",
        "\n",
        "  # check to see if the regression node should be added\n",
        "  if regress:\n",
        "    x = Dense(1, activation=\"linear\")(x)\n",
        "\n",
        "  # construct the CNN\n",
        "  model = Model(inputs, x)\n",
        "\n",
        "  # return the CNN\n",
        "  return model"
      ],
      "metadata": {
        "id": "9xKzTleyD6MI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the necessary packages\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import argparse\n",
        "import locale\n",
        "import os\n",
        "\n",
        "# construct the path to the input .txt file that contains information\n",
        "# on each house in the dataset and then load the dataset\n",
        "print(\"[INFO] loading house attributes...\")\n",
        "inputPath_df = \"/content/Houses-dataset/Houses Dataset/HousesInfo.txt\"\n",
        "df = load_house_attributes(inputPath= inputPath_df)\n",
        "\n",
        "# load the house images and then scale the pixel intensities to the\n",
        "# range [0, 1]\n",
        "print(\"[INFO] loading house images...\")\n",
        "inputPath = \"/content/Houses-dataset/Houses Dataset\"\n",
        "images = load_house_images(df, inputPath)\n",
        "images = images / 255.0\n",
        "\n",
        "# partition the data into training and testing splits using 80% of\n",
        "# the data for training and the remaining 20% for testing\n",
        "(trainAttrX, testAttrX, trainImagesX, testImagesX) = train_test_split(df, images, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "# find the largest house price in the training set and use it to\n",
        "# scale our house prices to the range [0, 1] (will lead to better training and convergence)\n",
        "maxPrice = trainAttrX[\"price\"].max()\n",
        "trainY = trainAttrX[\"price\"] / maxPrice\n",
        "testY = testAttrX[\"price\"] / maxPrice\n",
        "\n",
        "# create our Convolutional Neural Network and then compile the model\n",
        "# using mean absolute percentage error as our loss, implying that we\n",
        "# seek to minimize the absolute percentage difference between our\n",
        "# price *predictions* and the *actual prices*\n",
        "model = create_cnn(64, 64, 3, regress=True)\n",
        "model.compile(loss=\"mean_absolute_percentage_error\", optimizer=\"Adam\")\n",
        "\n",
        "# train the model\n",
        "print(\"[INFO] training model...\")\n",
        "#callback_reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss')\n",
        "\n",
        "model.fit(x=trainImagesX, y=trainY,\n",
        "    validation_data=(testImagesX, testY),\n",
        "    epochs=200, batch_size=8)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJIZ3w36SQxg",
        "outputId": "8d6d07b7-ae9f-4b9b-d9f5-99aa5347347c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] loading house attributes...\n",
            "[INFO] loading house images...\n",
            "[INFO] training model...\n",
            "Epoch 1/200\n",
            "37/37 [==============================] - 13s 18ms/step - loss: 1963.3849 - val_loss: 2048.5405\n",
            "Epoch 2/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 1571.0656 - val_loss: 4780.1367\n",
            "Epoch 3/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 1035.9656 - val_loss: 5742.0044\n",
            "Epoch 4/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 671.7953 - val_loss: 7352.5327\n",
            "Epoch 5/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 504.6903 - val_loss: 9308.5938\n",
            "Epoch 6/200\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 427.6508 - val_loss: 9661.9346\n",
            "Epoch 7/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 269.7313 - val_loss: 8326.3643\n",
            "Epoch 8/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 329.0517 - val_loss: 6586.5361\n",
            "Epoch 9/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 269.4047 - val_loss: 4076.1956\n",
            "Epoch 10/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 279.5126 - val_loss: 2116.9702\n",
            "Epoch 11/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 311.7828 - val_loss: 1404.2107\n",
            "Epoch 12/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 203.0743 - val_loss: 1566.8755\n",
            "Epoch 13/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 195.3372 - val_loss: 182.6883\n",
            "Epoch 14/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 215.6008 - val_loss: 96.9908\n",
            "Epoch 15/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 174.7743 - val_loss: 133.2234\n",
            "Epoch 16/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 199.7711 - val_loss: 126.0858\n",
            "Epoch 17/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 165.1498 - val_loss: 77.7802\n",
            "Epoch 18/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 126.4766 - val_loss: 70.9066\n",
            "Epoch 19/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 135.7205 - val_loss: 113.4409\n",
            "Epoch 20/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 106.1276 - val_loss: 98.3436\n",
            "Epoch 21/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 121.5520 - val_loss: 117.5734\n",
            "Epoch 22/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 104.4331 - val_loss: 111.1320\n",
            "Epoch 23/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 104.2260 - val_loss: 83.4370\n",
            "Epoch 24/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 109.6837 - val_loss: 77.8880\n",
            "Epoch 25/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 92.9160 - val_loss: 88.7125\n",
            "Epoch 26/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 85.7799 - val_loss: 89.0824\n",
            "Epoch 27/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 91.0194 - val_loss: 110.2733\n",
            "Epoch 28/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 97.2839 - val_loss: 77.9736\n",
            "Epoch 29/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 85.2511 - val_loss: 87.0536\n",
            "Epoch 30/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 85.9470 - val_loss: 101.9434\n",
            "Epoch 31/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 85.9748 - val_loss: 93.5424\n",
            "Epoch 32/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 84.1611 - val_loss: 74.5686\n",
            "Epoch 33/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 79.3380 - val_loss: 73.0762\n",
            "Epoch 34/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 81.6823 - val_loss: 75.7784\n",
            "Epoch 35/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 72.5370 - val_loss: 71.9725\n",
            "Epoch 36/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 80.0086 - val_loss: 64.4534\n",
            "Epoch 37/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 82.8216 - val_loss: 101.4468\n",
            "Epoch 38/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 75.4675 - val_loss: 101.4715\n",
            "Epoch 39/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 82.4344 - val_loss: 84.0320\n",
            "Epoch 40/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 69.0368 - val_loss: 91.2550\n",
            "Epoch 41/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 72.1678 - val_loss: 72.4251\n",
            "Epoch 42/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 70.4355 - val_loss: 64.7933\n",
            "Epoch 43/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 73.5537 - val_loss: 82.4210\n",
            "Epoch 44/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 71.3130 - val_loss: 68.6392\n",
            "Epoch 45/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 68.4216 - val_loss: 69.6467\n",
            "Epoch 46/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 70.8593 - val_loss: 72.1940\n",
            "Epoch 47/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 66.6885 - val_loss: 66.4837\n",
            "Epoch 48/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 66.5778 - val_loss: 77.3502\n",
            "Epoch 49/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 71.0459 - val_loss: 77.7724\n",
            "Epoch 50/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 75.3502 - val_loss: 89.4933\n",
            "Epoch 51/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 71.1424 - val_loss: 71.4198\n",
            "Epoch 52/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 76.2960 - val_loss: 69.2319\n",
            "Epoch 53/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 70.0041 - val_loss: 62.9039\n",
            "Epoch 54/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 69.6897 - val_loss: 66.0997\n",
            "Epoch 55/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 64.6038 - val_loss: 65.7089\n",
            "Epoch 56/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 68.3222 - val_loss: 62.0314\n",
            "Epoch 57/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 67.9869 - val_loss: 65.1998\n",
            "Epoch 58/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 64.9982 - val_loss: 59.1456\n",
            "Epoch 59/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 76.2914 - val_loss: 65.7151\n",
            "Epoch 60/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 63.4144 - val_loss: 63.1887\n",
            "Epoch 61/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 66.5236 - val_loss: 64.6398\n",
            "Epoch 62/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 62.7777 - val_loss: 60.5885\n",
            "Epoch 63/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 61.7721 - val_loss: 64.3221\n",
            "Epoch 64/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 62.0647 - val_loss: 67.7966\n",
            "Epoch 65/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 65.6494 - val_loss: 62.7180\n",
            "Epoch 66/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 67.7461 - val_loss: 58.8650\n",
            "Epoch 67/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 58.9166 - val_loss: 57.5494\n",
            "Epoch 68/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 60.5498 - val_loss: 66.0642\n",
            "Epoch 69/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 64.2966 - val_loss: 67.0856\n",
            "Epoch 70/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 64.3190 - val_loss: 65.0437\n",
            "Epoch 71/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 59.0919 - val_loss: 69.9702\n",
            "Epoch 72/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 63.2394 - val_loss: 63.2968\n",
            "Epoch 73/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 63.6897 - val_loss: 60.0730\n",
            "Epoch 74/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 61.5098 - val_loss: 67.4426\n",
            "Epoch 75/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 58.9180 - val_loss: 60.0993\n",
            "Epoch 76/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 57.1606 - val_loss: 67.2110\n",
            "Epoch 77/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 56.5510 - val_loss: 67.6018\n",
            "Epoch 78/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 57.6952 - val_loss: 62.2012\n",
            "Epoch 79/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 57.3732 - val_loss: 60.8666\n",
            "Epoch 80/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 59.1477 - val_loss: 70.5315\n",
            "Epoch 81/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 59.3628 - val_loss: 70.2199\n",
            "Epoch 82/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 54.0254 - val_loss: 72.4180\n",
            "Epoch 83/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 53.8858 - val_loss: 64.8185\n",
            "Epoch 84/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 56.6678 - val_loss: 74.4460\n",
            "Epoch 85/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 58.7047 - val_loss: 66.9015\n",
            "Epoch 86/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 56.2768 - val_loss: 66.0245\n",
            "Epoch 87/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 59.2592 - val_loss: 68.5464\n",
            "Epoch 88/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 53.9784 - val_loss: 70.0662\n",
            "Epoch 89/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 54.6310 - val_loss: 68.5054\n",
            "Epoch 90/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 57.9492 - val_loss: 63.9486\n",
            "Epoch 91/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 55.5724 - val_loss: 58.6962\n",
            "Epoch 92/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 54.6938 - val_loss: 62.7337\n",
            "Epoch 93/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 58.6799 - val_loss: 55.2723\n",
            "Epoch 94/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 51.6673 - val_loss: 57.9247\n",
            "Epoch 95/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 55.1903 - val_loss: 58.3656\n",
            "Epoch 96/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 56.6919 - val_loss: 61.7259\n",
            "Epoch 97/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 55.0781 - val_loss: 58.3655\n",
            "Epoch 98/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 55.8801 - val_loss: 56.1453\n",
            "Epoch 99/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 56.9265 - val_loss: 77.2837\n",
            "Epoch 100/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 53.4228 - val_loss: 81.8353\n",
            "Epoch 101/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 59.6373 - val_loss: 61.0063\n",
            "Epoch 102/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 52.7825 - val_loss: 56.4301\n",
            "Epoch 103/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 51.4229 - val_loss: 56.1841\n",
            "Epoch 104/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 49.0047 - val_loss: 55.9749\n",
            "Epoch 105/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 52.0798 - val_loss: 55.7595\n",
            "Epoch 106/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 48.8464 - val_loss: 55.8992\n",
            "Epoch 107/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 63.7181 - val_loss: 61.3852\n",
            "Epoch 108/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 51.0657 - val_loss: 63.3341\n",
            "Epoch 109/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 58.0026 - val_loss: 59.6684\n",
            "Epoch 110/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 53.0862 - val_loss: 63.5544\n",
            "Epoch 111/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 47.5523 - val_loss: 62.7436\n",
            "Epoch 112/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 53.4953 - val_loss: 65.2004\n",
            "Epoch 113/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 49.7952 - val_loss: 98.8804\n",
            "Epoch 114/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 50.7393 - val_loss: 66.7840\n",
            "Epoch 115/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 51.3028 - val_loss: 60.0699\n",
            "Epoch 116/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 46.4537 - val_loss: 63.4104\n",
            "Epoch 117/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 49.8548 - val_loss: 62.5644\n",
            "Epoch 118/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 52.6407 - val_loss: 68.3770\n",
            "Epoch 119/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 51.7874 - val_loss: 61.5078\n",
            "Epoch 120/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 48.3777 - val_loss: 53.8086\n",
            "Epoch 121/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 47.0040 - val_loss: 52.6464\n",
            "Epoch 122/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 50.6973 - val_loss: 63.7772\n",
            "Epoch 123/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 49.6104 - val_loss: 60.6350\n",
            "Epoch 124/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 47.2935 - val_loss: 59.0349\n",
            "Epoch 125/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 43.5860 - val_loss: 55.4364\n",
            "Epoch 126/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 46.3209 - val_loss: 54.7589\n",
            "Epoch 127/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 44.6912 - val_loss: 54.5658\n",
            "Epoch 128/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 47.9557 - val_loss: 62.8219\n",
            "Epoch 129/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 45.4265 - val_loss: 62.5496\n",
            "Epoch 130/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 48.6091 - val_loss: 58.6095\n",
            "Epoch 131/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 48.5357 - val_loss: 64.9419\n",
            "Epoch 132/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 46.4520 - val_loss: 64.8851\n",
            "Epoch 133/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 46.1640 - val_loss: 60.9070\n",
            "Epoch 134/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 46.1413 - val_loss: 65.5146\n",
            "Epoch 135/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 44.7953 - val_loss: 62.4638\n",
            "Epoch 136/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 44.6915 - val_loss: 63.2435\n",
            "Epoch 137/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 42.0134 - val_loss: 61.0209\n",
            "Epoch 138/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 45.7973 - val_loss: 52.3696\n",
            "Epoch 139/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 49.1487 - val_loss: 59.9622\n",
            "Epoch 140/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 47.0011 - val_loss: 64.6997\n",
            "Epoch 141/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 44.8410 - val_loss: 66.2176\n",
            "Epoch 142/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 46.0074 - val_loss: 59.0893\n",
            "Epoch 143/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 44.5777 - val_loss: 58.0267\n",
            "Epoch 144/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 44.7020 - val_loss: 50.7955\n",
            "Epoch 145/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 45.0171 - val_loss: 47.7412\n",
            "Epoch 146/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 46.1639 - val_loss: 66.5895\n",
            "Epoch 147/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 41.2800 - val_loss: 55.7777\n",
            "Epoch 148/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 44.3803 - val_loss: 59.0403\n",
            "Epoch 149/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 45.3011 - val_loss: 54.9242\n",
            "Epoch 150/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 39.3887 - val_loss: 54.1625\n",
            "Epoch 151/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 46.0733 - val_loss: 53.1921\n",
            "Epoch 152/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 46.0317 - val_loss: 56.7025\n",
            "Epoch 153/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 47.4072 - val_loss: 62.6387\n",
            "Epoch 154/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 45.7105 - val_loss: 58.9257\n",
            "Epoch 155/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 42.6409 - val_loss: 55.8736\n",
            "Epoch 156/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 44.5879 - val_loss: 51.1467\n",
            "Epoch 157/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 43.1224 - val_loss: 45.0378\n",
            "Epoch 158/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 43.9005 - val_loss: 62.1892\n",
            "Epoch 159/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 42.7896 - val_loss: 68.8442\n",
            "Epoch 160/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 42.2419 - val_loss: 59.8884\n",
            "Epoch 161/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 43.3342 - val_loss: 52.6184\n",
            "Epoch 162/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 42.0843 - val_loss: 56.0757\n",
            "Epoch 163/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 41.9891 - val_loss: 62.8690\n",
            "Epoch 164/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 42.1778 - val_loss: 56.1867\n",
            "Epoch 165/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 39.3580 - val_loss: 49.4891\n",
            "Epoch 166/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 44.9256 - val_loss: 49.9694\n",
            "Epoch 167/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 44.4770 - val_loss: 124.0196\n",
            "Epoch 168/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 45.3819 - val_loss: 74.6227\n",
            "Epoch 169/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 44.4043 - val_loss: 60.5352\n",
            "Epoch 170/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 45.0922 - val_loss: 54.2729\n",
            "Epoch 171/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 44.9740 - val_loss: 48.7040\n",
            "Epoch 172/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 44.4774 - val_loss: 171.5797\n",
            "Epoch 173/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 47.9931 - val_loss: 60.1702\n",
            "Epoch 174/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 45.1868 - val_loss: 58.0074\n",
            "Epoch 175/200\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 43.7325 - val_loss: 53.0717\n",
            "Epoch 176/200\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 46.0449 - val_loss: 56.8160\n",
            "Epoch 177/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 43.9260 - val_loss: 54.3893\n",
            "Epoch 178/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 42.0330 - val_loss: 52.6904\n",
            "Epoch 179/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 40.4272 - val_loss: 54.0570\n",
            "Epoch 180/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 41.9819 - val_loss: 55.9262\n",
            "Epoch 181/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 43.2479 - val_loss: 61.6259\n",
            "Epoch 182/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 43.3725 - val_loss: 72.3089\n",
            "Epoch 183/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 40.4198 - val_loss: 60.8524\n",
            "Epoch 184/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 40.9405 - val_loss: 64.3106\n",
            "Epoch 185/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 41.9535 - val_loss: 63.0111\n",
            "Epoch 186/200\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 39.5443 - val_loss: 63.0558\n",
            "Epoch 187/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 41.8431 - val_loss: 58.0960\n",
            "Epoch 188/200\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 44.5950 - val_loss: 65.8889\n",
            "Epoch 189/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 42.5855 - val_loss: 65.6382\n",
            "Epoch 190/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 38.9121 - val_loss: 58.1964\n",
            "Epoch 191/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 41.4320 - val_loss: 52.8853\n",
            "Epoch 192/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 42.2412 - val_loss: 51.2523\n",
            "Epoch 193/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 37.7233 - val_loss: 53.1759\n",
            "Epoch 194/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 44.3806 - val_loss: 56.9716\n",
            "Epoch 195/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 41.4608 - val_loss: 56.7707\n",
            "Epoch 196/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 34.8728 - val_loss: 53.8031\n",
            "Epoch 197/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 42.3212 - val_loss: 59.3645\n",
            "Epoch 198/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 37.6229 - val_loss: 256.0172\n",
            "Epoch 199/200\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 42.5755 - val_loss: 6782.5054\n",
            "Epoch 200/200\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 44.4035 - val_loss: 86.2606\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x78acdcc446a0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make predictions on the testing data\n",
        "print(\"[INFO] predicting house prices...\")\n",
        "preds = model.predict(testImagesX)\n",
        "# compute the difference between the *predicted* house prices and the\n",
        "# *actual* house prices, then compute the percentage difference and\n",
        "# the absolute percentage difference\n",
        "diff = preds.flatten() - testY\n",
        "percentDiff = (diff / testY) * 100\n",
        "absPercentDiff = np.abs(percentDiff)\n",
        "# compute the mean and standard deviation of the absolute percentage\n",
        "# difference\n",
        "mean = np.mean(absPercentDiff)\n",
        "std = np.std(absPercentDiff)\n",
        "# finally, show some statistics on our model\n",
        "locale.setlocale(locale.LC_ALL, \"en_US.UTF-8\")\n",
        "print(\"[INFO] avg. house price: {}, std house price: {}\".format(\n",
        "\tlocale.currency(df[\"price\"].mean(), grouping=True),\n",
        "\tlocale.currency(df[\"price\"].std(), grouping=True)))\n",
        "print(\"[INFO] mean: {:.2f}%, std: {:.2f}%\".format(mean, std))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLikslKnWzZ6",
        "outputId": "84d11d95-4718-4577-821f-d837deef2e3c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] predicting house prices...\n",
            "3/3 [==============================] - 0s 43ms/step\n",
            "[INFO] avg. house price: $533,388.27, std house price: $493,403.08\n",
            "[INFO] mean: 86.26%, std: 103.72%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Keras: Multiple Inputs and Mixed Data**"
      ],
      "metadata": {
        "id": "LQjOQCESbYQK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pyimagesearch.com/wp-content/uploads/2019/02/keras_multi_input_header.png\n",
        "\n",
        "https://pyimagesearch.com/wp-content/uploads/2019/02/keras_multi_input_mixed_data.png\n",
        "\n",
        "https://pyimagesearch.com/wp-content/uploads/2019/02/keras_multi_input_arch.png"
      ],
      "metadata": {
        "id": "lMurxCpm8cOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/emanhamed/Houses-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Or7qGtpbZYJ",
        "outputId": "cbf82b1b-6bab-4630-b531-e593f4346070"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Houses-dataset'...\n",
            "remote: Enumerating objects: 2166, done.\u001b[K\n",
            "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 2166 (delta 0), reused 0 (delta 0), pack-reused 2165\u001b[K\n",
            "Receiving objects: 100% (2166/2166), 176.26 MiB | 14.70 MiB/s, done.\n",
            "Resolving deltas: 100% (20/20), done.\n",
            "Updating files: 100% (2144/2144), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import the necessary packages\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "\n",
        "def load_house_attributes(inputPath):\n",
        "\t# initialize the list of column names in the CSV file and then load it using Pandas\n",
        "\tcols = [\"bedrooms\", \"bathrooms\", \"area\", \"zipcode\", \"price\"]\n",
        "\tdf = pd.read_csv(inputPath, sep=\" \", header=None, names=cols)\n",
        "\n",
        "\t# determine (1) the unique zip codes and (2) the number of data points with each zip code\n",
        "\tzipcodes = df[\"zipcode\"].value_counts().keys().tolist()\n",
        "\tcounts = df[\"zipcode\"].value_counts().tolist()\n",
        "\n",
        "\t# loop over each of the unique zip codes and their corresponding count\n",
        "\tfor (zipcode, count) in zip(zipcodes, counts):\n",
        "\t\t# the zip code counts for our housing dataset is *extremely*\n",
        "\t\t# unbalanced (some only having 1 or 2 houses per zip code)\n",
        "\t\t# so let's sanitize our data by removing any houses with less\n",
        "\t\t# than 25 houses per zip code\n",
        "\t\tif count < 25:\n",
        "\t\t\tidxs = df[df[\"zipcode\"] == zipcode].index\n",
        "\t\t\tdf.drop(idxs, inplace=True)\n",
        "\n",
        "\t# return the data frame\n",
        "\treturn df"
      ],
      "metadata": {
        "id": "HF9gPi71EEYz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputPath = \"/content/Houses-dataset/Houses Dataset/HousesInfo.txt\"\n",
        "\n",
        "df = pd.read_csv(inputPath, sep = \" \", names=[\"bedrooms\", \"bathrooms\", \"area\", \"zipcode\", \"price\"] )\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "mOz3yz4MEQK1",
        "outputId": "f3265d66-3961-42e9-d71d-6039a71fa7dc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     bedrooms  bathrooms  area  zipcode   price\n",
              "0           4        4.0  4053    85255  869500\n",
              "1           4        3.0  3343    36372  865200\n",
              "2           3        4.0  3923    85266  889000\n",
              "3           5        5.0  4022    85262  910000\n",
              "4           3        4.0  4116    85266  971226\n",
              "..        ...        ...   ...      ...     ...\n",
              "530         5        2.0  2066    94531  399900\n",
              "531         4        3.5  9536    94531  460000\n",
              "532         3        2.0  2014    94531  407000\n",
              "533         4        3.0  2312    94531  419000\n",
              "534         5        3.0  3796    94531  615000\n",
              "\n",
              "[535 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-6764bfeb-20ed-4789-b56d-de685b1a85a5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>area</th>\n",
              "      <th>zipcode</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4053</td>\n",
              "      <td>85255</td>\n",
              "      <td>869500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3343</td>\n",
              "      <td>36372</td>\n",
              "      <td>865200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3923</td>\n",
              "      <td>85266</td>\n",
              "      <td>889000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4022</td>\n",
              "      <td>85262</td>\n",
              "      <td>910000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4116</td>\n",
              "      <td>85266</td>\n",
              "      <td>971226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>530</th>\n",
              "      <td>5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2066</td>\n",
              "      <td>94531</td>\n",
              "      <td>399900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>531</th>\n",
              "      <td>4</td>\n",
              "      <td>3.5</td>\n",
              "      <td>9536</td>\n",
              "      <td>94531</td>\n",
              "      <td>460000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>532</th>\n",
              "      <td>3</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2014</td>\n",
              "      <td>94531</td>\n",
              "      <td>407000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>533</th>\n",
              "      <td>4</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2312</td>\n",
              "      <td>94531</td>\n",
              "      <td>419000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>534</th>\n",
              "      <td>5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3796</td>\n",
              "      <td>94531</td>\n",
              "      <td>615000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>535 rows Ã— 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6764bfeb-20ed-4789-b56d-de685b1a85a5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-2bb60a1d-b0f8-4bc1-b4e0-ab5247dae515\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2bb60a1d-b0f8-4bc1-b4e0-ab5247dae515')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-2bb60a1d-b0f8-4bc1-b4e0-ab5247dae515 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6764bfeb-20ed-4789-b56d-de685b1a85a5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6764bfeb-20ed-4789-b56d-de685b1a85a5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_house_attributes(df, train, test):\n",
        "  # initialize the column names of the continuous data\n",
        "  continuous = [\"bedrooms\", \"bathrooms\", \"area\"]\n",
        "\n",
        "  # perform min-max scaling each continuous feature column to the range [0, 1]\n",
        "  cs = MinMaxScaler()\n",
        "  trainContinuous = cs.fit_transform(train[continuous])\n",
        "  testContinuous = cs.transform(test[continuous])\n",
        "\n",
        "  # one-hot encode the zip code categorical data (by definition of\n",
        "  # one-hot encoing, all output features are now in the range [0, 1])\n",
        "  zipBinarizer = LabelBinarizer().fit(df[\"zipcode\"])\n",
        "  trainCategorical = zipBinarizer.transform(train[\"zipcode\"])\n",
        "  testCategorical = zipBinarizer.transform(test[\"zipcode\"])\n",
        "\n",
        "  # construct our training and testing data points by concatenating\n",
        "  # the categorical features with the continuous features\n",
        "  trainX = np.hstack([trainCategorical, trainContinuous])\n",
        "  testX = np.hstack([testCategorical, testContinuous])\n",
        "\n",
        "  # return the concatenated training and testing data\n",
        "  return (trainX, testX)"
      ],
      "metadata": {
        "id": "nBODuLIXMo8q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputPath = \"/content/Houses-dataset/Houses Dataset\"\n",
        "\n",
        "def load_house_images(df, inputPath):\n",
        "\t# initialize our images array (i.e., the house images themselves)\n",
        "\timages = []\n",
        "\n",
        "\t# loop over the indexes of the houses\n",
        "\tfor i in df.index.values:\n",
        "\t\t# find the four images for the house and sort the file paths,\n",
        "\t\t# ensuring the four are always in the *same order*\n",
        "\t\tbasePath = os.path.sep.join([inputPath, \"{}_*\".format(i + 1)])\n",
        "\t\thousePaths = sorted(list(glob.glob(basePath)))\n",
        "\n",
        "    # initialize our list of input images along with the output image\n",
        "\t\t# after *combining* the four input images\n",
        "\t\tinputImages = []\n",
        "\t\toutputImage = np.zeros((64, 64, 3), dtype=\"uint8\")\n",
        "\n",
        "\t\t# loop over the input house paths\n",
        "\t\tfor housePath in housePaths:\n",
        "\t\t\t# load the input image, resize it to be 32 32, and then\n",
        "\t\t\t# update the list of input images\n",
        "\t\t\timage = cv2.imread(housePath)\n",
        "\t\t\timage = cv2.resize(image, (32, 32))\n",
        "\t\t\tinputImages.append(image)\n",
        "\n",
        "    # tile the four input images in the output image such the first\n",
        "\t\t# image goes in the top-right corner, the second image in the\n",
        "\t\t# top-left corner, the third image in the bottom-right corner,\n",
        "\t\t# and the final image in the bottom-left corner\n",
        "\t\toutputImage[0:32, 0:32] = inputImages[0]\n",
        "\t\toutputImage[0:32, 32:64] = inputImages[1]\n",
        "\t\toutputImage[32:64, 32:64] = inputImages[2]\n",
        "\t\toutputImage[32:64, 0:32] = inputImages[3]\n",
        "\n",
        "\n",
        "\n",
        "    # another way to attach images:\n",
        "    # up = np.hstack([inputImages[0],inputImages[1]])\n",
        "    # down = np.hstack([inputImages[3],inputImages[2]])\n",
        "    # img = np.vstack([up, down])\n",
        "    # img.shape\n",
        "\n",
        "\n",
        "\n",
        "\t\t# add the tiled image to our set of images the network will be\n",
        "\t\t# trained on\n",
        "\t\timages.append(outputImage)\n",
        "\n",
        "\t# return our set of images\n",
        "\treturn np.array(images)"
      ],
      "metadata": {
        "id": "wsXteVrONxli"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Activation, Dropout, Dense, Flatten, Input\n",
        "\n",
        "\n",
        "def create_mlp(dim, regress=False):\n",
        "  # define our MLP network\n",
        "  model = Sequential()\n",
        "  model.add(Dense(8, input_dim=dim, activation=\"relu\"))\n",
        "  model.add(Dense(4, activation=\"relu\"))\n",
        "\n",
        "\n",
        "  # check to see if the regression node should be added\n",
        "  if regress:\n",
        "    model.add(Dense(1, activation=\"linear\"))\n",
        "  # return our model\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "cQ8u1eawOqhk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Activation, Dropout, Dense, Flatten, Input\n",
        "\n",
        "\n",
        "def create_cnn(width, height, depth, filters=(16, 32, 64), regress=False):\n",
        "  # initialize the input shape and channel dimension, assuming\n",
        "  # TensorFlow/channels-last ordering\n",
        "  inputShape = (height, width, depth)\n",
        "  chanDim = -1\n",
        "\n",
        "  # define the model input\n",
        "  inputs = Input(shape=inputShape, batch_size=8)\n",
        "\n",
        "  # loop over the number of filters\n",
        "  for (i, f) in enumerate(filters):\n",
        "\n",
        "    # if this is the first CONV layer then set the input appropriately\n",
        "    if i == 0:\n",
        "      x = inputs\n",
        "\n",
        "    # CONV => RELU => BN => POOL\n",
        "    x = Conv2D(f, (3, 3), padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization(axis=chanDim)(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "  # flatten the volume, then FC => RELU => BN => DROPOUT\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(16)(x)\n",
        "  x = Activation(\"relu\")(x)\n",
        "  x = BatchNormalization(axis=chanDim)(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "\n",
        "  # apply another FC layer, this one to match the number of nodes\n",
        "  # coming out of the MLP\n",
        "  x = Dense(4)(x)\n",
        "  x = Activation(\"relu\")(x)\n",
        "\n",
        "  # check to see if the regression node should be added\n",
        "  if regress:\n",
        "    x = Dense(1, activation=\"linear\")(x)\n",
        "\n",
        "  # construct the CNN\n",
        "  model = Model(inputs, x)\n",
        "\n",
        "  # return the CNN\n",
        "  return model"
      ],
      "metadata": {
        "id": "HYSZHLmlO9O9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multiple inputs with Keras**"
      ],
      "metadata": {
        "id": "auGPXhESPXU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the necessary packages\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import concatenate\n",
        "import numpy as np\n",
        "import argparse\n",
        "import locale\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "print(\"[INFO] loading house attributes...\")\n",
        "inputPath = \"/content/Houses-dataset/Houses Dataset/HousesInfo.txt\"\n",
        "df = load_house_attributes(inputPath)\n",
        "\n",
        "print(\"[INFO] loading house images...\")\n",
        "inputPath_images = \"/content/Houses-dataset/Houses Dataset\"\n",
        "images = load_house_images(df, inputPath_images)\n",
        "images = images / 255.0\n",
        "\n",
        "\n",
        "# partition the data into training and testing splits using 75% of the data for training and the remaining 25% for testing\n",
        "(trainAttrX, testAttrX, trainImagesX, testImagesX) = train_test_split(df, images, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "# convert label range between [0, 1]\n",
        "maxPrice = trainAttrX[\"price\"].max()\n",
        "trainY = trainAttrX[\"price\"] / maxPrice\n",
        "testY = testAttrX[\"price\"] / maxPrice\n",
        "\n",
        "\n",
        "\n",
        "# process the house attributes data by performing min-max scaling\n",
        "# on continuous features, one-hot encoding on categorical features,\n",
        "# and then finally concatenating them together\n",
        "(trainAttrX, testAttrX) = process_house_attributes(df, trainAttrX, testAttrX)\n",
        "\n",
        "\n",
        "\n",
        "# create the MLP and CNN models\n",
        "mlp = create_mlp(trainAttrX.shape[1], regress=False)\n",
        "cnn = create_cnn(64, 64, 3, regress=False)\n",
        "\n",
        "# create the input to our final set of layers as the *output* of both the MLP and CNN\n",
        "combinedInput = concatenate([mlp.output, cnn.output])\n",
        "\n",
        "# our final FC layer head will have two dense layers, the final one\n",
        "# being our regression head\n",
        "x = Dense(4, activation=\"relu\")(combinedInput)\n",
        "x = Dense(1, activation=\"linear\")(x)\n",
        "\n",
        "# our final model will accept categorical/numerical data on the MLP\n",
        "# input and images on the CNN input, outputting a single value (the predicted price of the house)\n",
        "model = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
        "\n",
        "\n",
        "\n",
        "model.compile(loss=\"mean_absolute_percentage_error\", optimizer='Adam')\n",
        "\n",
        "# train the model\n",
        "model.fit(\n",
        "\tx=[trainAttrX, trainImagesX], y=trainY,\n",
        "\tvalidation_data=([testAttrX, testImagesX], testY),\n",
        "\tepochs=200, batch_size=8)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7Klyp8YPWnu",
        "outputId": "85f7b80b-0767-4736-8f35-1eeb3785332d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] loading house attributes...\n",
            "[INFO] loading house images...\n",
            "Epoch 1/200\n",
            "34/34 [==============================] - 15s 36ms/step - loss: 1432.6733 - val_loss: 206.7444\n",
            "Epoch 2/200\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 662.3272 - val_loss: 312.0533\n",
            "Epoch 3/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 392.1815 - val_loss: 978.6492\n",
            "Epoch 4/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 318.6018 - val_loss: 926.0029\n",
            "Epoch 5/200\n",
            "34/34 [==============================] - 0s 12ms/step - loss: 256.9643 - val_loss: 1121.7869\n",
            "Epoch 6/200\n",
            "34/34 [==============================] - 0s 12ms/step - loss: 196.3510 - val_loss: 1259.2904\n",
            "Epoch 7/200\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 168.5683 - val_loss: 1244.3789\n",
            "Epoch 8/200\n",
            "34/34 [==============================] - 0s 13ms/step - loss: 162.0541 - val_loss: 646.7910\n",
            "Epoch 9/200\n",
            "34/34 [==============================] - 0s 13ms/step - loss: 169.9431 - val_loss: 498.9563\n",
            "Epoch 10/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 131.3915 - val_loss: 458.3108\n",
            "Epoch 11/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 114.7363 - val_loss: 342.5815\n",
            "Epoch 12/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 109.7057 - val_loss: 97.7349\n",
            "Epoch 13/200\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 103.7820 - val_loss: 67.2477\n",
            "Epoch 14/200\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 94.2041 - val_loss: 70.1531\n",
            "Epoch 15/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 86.6405 - val_loss: 67.0604\n",
            "Epoch 16/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 94.4205 - val_loss: 106.4982\n",
            "Epoch 17/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 98.2388 - val_loss: 72.1318\n",
            "Epoch 18/200\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 83.0466 - val_loss: 85.5383\n",
            "Epoch 19/200\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 87.8108 - val_loss: 72.0969\n",
            "Epoch 20/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 74.1212 - val_loss: 59.3420\n",
            "Epoch 21/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 72.6866 - val_loss: 63.4106\n",
            "Epoch 22/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 76.1929 - val_loss: 55.6652\n",
            "Epoch 23/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 65.8538 - val_loss: 63.3749\n",
            "Epoch 24/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 72.0157 - val_loss: 56.5219\n",
            "Epoch 25/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 60.5139 - val_loss: 47.9581\n",
            "Epoch 26/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 60.6690 - val_loss: 53.1155\n",
            "Epoch 27/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 52.7678 - val_loss: 50.2543\n",
            "Epoch 28/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 54.9992 - val_loss: 52.5208\n",
            "Epoch 29/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 53.9551 - val_loss: 44.0043\n",
            "Epoch 30/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 47.6951 - val_loss: 46.9604\n",
            "Epoch 31/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 46.2486 - val_loss: 44.6772\n",
            "Epoch 32/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 46.8351 - val_loss: 43.6922\n",
            "Epoch 33/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 46.0817 - val_loss: 43.4397\n",
            "Epoch 34/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 42.2840 - val_loss: 41.1630\n",
            "Epoch 35/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 40.7522 - val_loss: 41.1610\n",
            "Epoch 36/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 38.0116 - val_loss: 37.6049\n",
            "Epoch 37/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 36.5750 - val_loss: 37.4213\n",
            "Epoch 38/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 39.4933 - val_loss: 39.7490\n",
            "Epoch 39/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 36.8917 - val_loss: 35.6734\n",
            "Epoch 40/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 34.7055 - val_loss: 34.1332\n",
            "Epoch 41/200\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 33.4644 - val_loss: 33.1644\n",
            "Epoch 42/200\n",
            "34/34 [==============================] - 0s 12ms/step - loss: 33.0471 - val_loss: 34.3499\n",
            "Epoch 43/200\n",
            "34/34 [==============================] - 0s 13ms/step - loss: 33.5579 - val_loss: 33.4635\n",
            "Epoch 44/200\n",
            "34/34 [==============================] - 0s 13ms/step - loss: 28.5966 - val_loss: 32.4703\n",
            "Epoch 45/200\n",
            "34/34 [==============================] - 0s 12ms/step - loss: 30.5875 - val_loss: 29.2388\n",
            "Epoch 46/200\n",
            "34/34 [==============================] - 0s 12ms/step - loss: 30.3479 - val_loss: 32.2087\n",
            "Epoch 47/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 32.4446 - val_loss: 32.3810\n",
            "Epoch 48/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 29.0077 - val_loss: 29.9848\n",
            "Epoch 49/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 29.1074 - val_loss: 29.0340\n",
            "Epoch 50/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 30.4136 - val_loss: 32.9094\n",
            "Epoch 51/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 29.0065 - val_loss: 28.9060\n",
            "Epoch 52/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 30.0373 - val_loss: 28.5619\n",
            "Epoch 53/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 30.8245 - val_loss: 30.5593\n",
            "Epoch 54/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 27.0978 - val_loss: 29.7817\n",
            "Epoch 55/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 28.0118 - val_loss: 27.2779\n",
            "Epoch 56/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 27.3337 - val_loss: 28.4139\n",
            "Epoch 57/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 31.1160 - val_loss: 29.2596\n",
            "Epoch 58/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 29.1123 - val_loss: 26.9670\n",
            "Epoch 59/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 26.2046 - val_loss: 28.2341\n",
            "Epoch 60/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 27.3208 - val_loss: 26.7699\n",
            "Epoch 61/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 24.0391 - val_loss: 26.4521\n",
            "Epoch 62/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 27.5518 - val_loss: 27.4663\n",
            "Epoch 63/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 26.2450 - val_loss: 26.7610\n",
            "Epoch 64/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 26.6299 - val_loss: 27.1647\n",
            "Epoch 65/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 26.3330 - val_loss: 25.4739\n",
            "Epoch 66/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 24.9453 - val_loss: 25.7483\n",
            "Epoch 67/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 24.9804 - val_loss: 26.7931\n",
            "Epoch 68/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 26.1745 - val_loss: 25.8363\n",
            "Epoch 69/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 25.7378 - val_loss: 27.0549\n",
            "Epoch 70/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 28.7034 - val_loss: 25.0021\n",
            "Epoch 71/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 25.8138 - val_loss: 27.6575\n",
            "Epoch 72/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 27.2577 - val_loss: 25.9037\n",
            "Epoch 73/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 26.2833 - val_loss: 25.3141\n",
            "Epoch 74/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 23.7964 - val_loss: 25.3964\n",
            "Epoch 75/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 25.5559 - val_loss: 26.5757\n",
            "Epoch 76/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 26.4738 - val_loss: 25.5019\n",
            "Epoch 77/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 24.8885 - val_loss: 28.0182\n",
            "Epoch 78/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 25.9838 - val_loss: 25.0010\n",
            "Epoch 79/200\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 26.2143 - val_loss: 24.4023\n",
            "Epoch 80/200\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 24.8863 - val_loss: 25.8668\n",
            "Epoch 81/200\n",
            "34/34 [==============================] - 0s 12ms/step - loss: 27.4071 - val_loss: 26.7427\n",
            "Epoch 82/200\n",
            "34/34 [==============================] - 0s 12ms/step - loss: 25.0021 - val_loss: 26.0126\n",
            "Epoch 83/200\n",
            "34/34 [==============================] - 0s 12ms/step - loss: 26.2562 - val_loss: 23.1622\n",
            "Epoch 84/200\n",
            "34/34 [==============================] - 0s 12ms/step - loss: 24.8045 - val_loss: 25.2672\n",
            "Epoch 85/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 25.2691 - val_loss: 25.6714\n",
            "Epoch 86/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 25.9121 - val_loss: 28.4965\n",
            "Epoch 87/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 24.9944 - val_loss: 24.4392\n",
            "Epoch 88/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 23.9172 - val_loss: 24.9545\n",
            "Epoch 89/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 25.6384 - val_loss: 25.5574\n",
            "Epoch 90/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 23.5849 - val_loss: 26.5930\n",
            "Epoch 91/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 25.3563 - val_loss: 24.2554\n",
            "Epoch 92/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 25.7235 - val_loss: 25.4738\n",
            "Epoch 93/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 25.2913 - val_loss: 23.0209\n",
            "Epoch 94/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 26.0587 - val_loss: 23.3157\n",
            "Epoch 95/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 23.2313 - val_loss: 22.6658\n",
            "Epoch 96/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 23.8366 - val_loss: 23.3616\n",
            "Epoch 97/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 24.9867 - val_loss: 23.5233\n",
            "Epoch 98/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 23.2299 - val_loss: 24.0319\n",
            "Epoch 99/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 24.7618 - val_loss: 24.6530\n",
            "Epoch 100/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 24.5963 - val_loss: 24.8507\n",
            "Epoch 101/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 24.8143 - val_loss: 25.2126\n",
            "Epoch 102/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 24.4500 - val_loss: 24.3217\n",
            "Epoch 103/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 24.2370 - val_loss: 24.0572\n",
            "Epoch 104/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 25.2603 - val_loss: 22.7641\n",
            "Epoch 105/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 22.3277 - val_loss: 22.5154\n",
            "Epoch 106/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 23.8241 - val_loss: 24.8656\n",
            "Epoch 107/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 25.3248 - val_loss: 24.8120\n",
            "Epoch 108/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 21.2425 - val_loss: 23.8874\n",
            "Epoch 109/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 23.9597 - val_loss: 27.3983\n",
            "Epoch 110/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 27.2642 - val_loss: 27.0987\n",
            "Epoch 111/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 26.5662 - val_loss: 23.9152\n",
            "Epoch 112/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 25.6688 - val_loss: 24.3780\n",
            "Epoch 113/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 27.5261 - val_loss: 24.9307\n",
            "Epoch 114/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 25.7018 - val_loss: 25.7543\n",
            "Epoch 115/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 22.4919 - val_loss: 23.8439\n",
            "Epoch 116/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 25.7716 - val_loss: 26.6912\n",
            "Epoch 117/200\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 25.3438 - val_loss: 24.8854\n",
            "Epoch 118/200\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 26.4030 - val_loss: 25.9284\n",
            "Epoch 119/200\n",
            "34/34 [==============================] - 0s 12ms/step - loss: 22.9197 - val_loss: 24.1928\n",
            "Epoch 120/200\n",
            "34/34 [==============================] - 0s 12ms/step - loss: 24.2828 - val_loss: 27.4493\n",
            "Epoch 121/200\n",
            "34/34 [==============================] - 0s 12ms/step - loss: 22.6332 - val_loss: 24.5201\n",
            "Epoch 122/200\n",
            "34/34 [==============================] - 0s 12ms/step - loss: 23.5701 - val_loss: 26.6433\n",
            "Epoch 123/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 22.4567 - val_loss: 23.6117\n",
            "Epoch 124/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 25.0650 - val_loss: 23.6776\n",
            "Epoch 125/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 23.0393 - val_loss: 22.7540\n",
            "Epoch 126/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 25.8465 - val_loss: 29.7749\n",
            "Epoch 127/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 22.3284 - val_loss: 22.6498\n",
            "Epoch 128/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 23.1917 - val_loss: 25.1747\n",
            "Epoch 129/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 22.0924 - val_loss: 23.1564\n",
            "Epoch 130/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 22.0818 - val_loss: 24.9445\n",
            "Epoch 131/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 23.1417 - val_loss: 23.8646\n",
            "Epoch 132/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 24.1492 - val_loss: 23.2846\n",
            "Epoch 133/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 22.1722 - val_loss: 25.0657\n",
            "Epoch 134/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 22.7160 - val_loss: 23.2569\n",
            "Epoch 135/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 21.4129 - val_loss: 23.8805\n",
            "Epoch 136/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 21.2308 - val_loss: 24.6736\n",
            "Epoch 137/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 22.9902 - val_loss: 22.8142\n",
            "Epoch 138/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 22.1412 - val_loss: 23.2844\n",
            "Epoch 139/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 22.8836 - val_loss: 25.2040\n",
            "Epoch 140/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 22.2667 - val_loss: 25.5237\n",
            "Epoch 141/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 21.8436 - val_loss: 26.5739\n",
            "Epoch 142/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 23.0264 - val_loss: 24.2976\n",
            "Epoch 143/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 20.9140 - val_loss: 23.0711\n",
            "Epoch 144/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 22.4664 - val_loss: 23.0907\n",
            "Epoch 145/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 22.5452 - val_loss: 24.2450\n",
            "Epoch 146/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 21.4429 - val_loss: 24.1018\n",
            "Epoch 147/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 23.0573 - val_loss: 24.8506\n",
            "Epoch 148/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 22.8078 - val_loss: 22.8298\n",
            "Epoch 149/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 24.7001 - val_loss: 24.6676\n",
            "Epoch 150/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 23.1517 - val_loss: 22.0985\n",
            "Epoch 151/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 22.4435 - val_loss: 23.7534\n",
            "Epoch 152/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 21.9435 - val_loss: 23.1156\n",
            "Epoch 153/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 23.1511 - val_loss: 24.0193\n",
            "Epoch 154/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 22.0339 - val_loss: 23.0139\n",
            "Epoch 155/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 23.4190 - val_loss: 23.7380\n",
            "Epoch 156/200\n",
            "34/34 [==============================] - 0s 12ms/step - loss: 23.4821 - val_loss: 24.3896\n",
            "Epoch 157/200\n",
            "34/34 [==============================] - 0s 12ms/step - loss: 22.5040 - val_loss: 25.6049\n",
            "Epoch 158/200\n",
            "34/34 [==============================] - 0s 12ms/step - loss: 20.8788 - val_loss: 26.3742\n",
            "Epoch 159/200\n",
            "34/34 [==============================] - 0s 13ms/step - loss: 20.9137 - val_loss: 25.1507\n",
            "Epoch 160/200\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 19.6369 - val_loss: 26.3769\n",
            "Epoch 161/200\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 22.5276 - val_loss: 23.4669\n",
            "Epoch 162/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 23.7642 - val_loss: 23.0481\n",
            "Epoch 163/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 22.1312 - val_loss: 24.5884\n",
            "Epoch 164/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 19.9886 - val_loss: 25.0131\n",
            "Epoch 165/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 20.6302 - val_loss: 24.8455\n",
            "Epoch 166/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 22.2262 - val_loss: 24.5280\n",
            "Epoch 167/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 19.7137 - val_loss: 24.1646\n",
            "Epoch 168/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 20.9419 - val_loss: 23.5580\n",
            "Epoch 169/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 21.4780 - val_loss: 25.6947\n",
            "Epoch 170/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 20.4011 - val_loss: 24.7225\n",
            "Epoch 171/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 20.7706 - val_loss: 22.8903\n",
            "Epoch 172/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 22.2842 - val_loss: 25.9429\n",
            "Epoch 173/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 22.4670 - val_loss: 23.0884\n",
            "Epoch 174/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 21.5065 - val_loss: 22.5029\n",
            "Epoch 175/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 22.9302 - val_loss: 23.8210\n",
            "Epoch 176/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 22.7670 - val_loss: 23.1282\n",
            "Epoch 177/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 22.1310 - val_loss: 25.6448\n",
            "Epoch 178/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 21.1616 - val_loss: 22.9004\n",
            "Epoch 179/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 21.2934 - val_loss: 23.3614\n",
            "Epoch 180/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 22.2509 - val_loss: 23.5171\n",
            "Epoch 181/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 24.8433 - val_loss: 23.6196\n",
            "Epoch 182/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 24.0011 - val_loss: 24.8568\n",
            "Epoch 183/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 21.5090 - val_loss: 23.2850\n",
            "Epoch 184/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 22.6466 - val_loss: 24.8921\n",
            "Epoch 185/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 21.1142 - val_loss: 22.5951\n",
            "Epoch 186/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 19.5429 - val_loss: 24.4689\n",
            "Epoch 187/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 21.2556 - val_loss: 22.9803\n",
            "Epoch 188/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 21.1316 - val_loss: 22.8727\n",
            "Epoch 189/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 21.1609 - val_loss: 26.3161\n",
            "Epoch 190/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 21.9318 - val_loss: 23.1597\n",
            "Epoch 191/200\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 22.1716 - val_loss: 22.6893\n",
            "Epoch 192/200\n",
            "34/34 [==============================] - 0s 10ms/step - loss: 20.3974 - val_loss: 23.8773\n",
            "Epoch 193/200\n",
            "34/34 [==============================] - 0s 12ms/step - loss: 21.7581 - val_loss: 22.4092\n",
            "Epoch 194/200\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 19.2766 - val_loss: 23.2166\n",
            "Epoch 195/200\n",
            "34/34 [==============================] - 0s 12ms/step - loss: 21.5408 - val_loss: 24.3351\n",
            "Epoch 196/200\n",
            "34/34 [==============================] - 0s 12ms/step - loss: 20.1646 - val_loss: 23.4158\n",
            "Epoch 197/200\n",
            "34/34 [==============================] - 0s 13ms/step - loss: 19.9654 - val_loss: 24.7574\n",
            "Epoch 198/200\n",
            "34/34 [==============================] - 0s 12ms/step - loss: 21.4501 - val_loss: 22.8653\n",
            "Epoch 199/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 19.9811 - val_loss: 22.2326\n",
            "Epoch 200/200\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 20.1952 - val_loss: 23.1206\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7d45df38f640>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make predictions on the testing data\n",
        "print(\"[INFO] predicting house prices...\")\n",
        "preds = model.predict([testAttrX, testImagesX])\n",
        "\n",
        "# compute the difference between the *predicted* house prices and the\n",
        "# *actual* house prices, then compute the percentage difference and\n",
        "# the absolute percentage difference\n",
        "diff = preds.flatten() - testY\n",
        "percentDiff = (diff / testY) * 100\n",
        "absPercentDiff = np.abs(percentDiff)\n",
        "# compute the mean and standard deviation of the absolute percentage\n",
        "# difference\n",
        "mean = np.mean(absPercentDiff)\n",
        "std = np.std(absPercentDiff)\n",
        "# finally, show some statistics on our model\n",
        "locale.setlocale(locale.LC_ALL, \"en_US.UTF-8\")\n",
        "print(\"[INFO] avg. house price: {}, std house price: {}\".format(\n",
        "\tlocale.currency(df[\"price\"].mean(), grouping=True),\n",
        "\tlocale.currency(df[\"price\"].std(), grouping=True)))\n",
        "print(\"[INFO] mean: {:.2f}%, std: {:.2f}%\".format(mean, std))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CyKayZETZq4",
        "outputId": "21f25950-7b8e-4b5c-dfc6-09f16c0c318c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] predicting house prices...\n",
            "3/3 [==============================] - 1s 88ms/step\n",
            "[INFO] avg. house price: $533,388.27, std house price: $493,403.08\n",
            "[INFO] mean: 23.12%, std: 19.60%\n"
          ]
        }
      ]
    }
  ]
}