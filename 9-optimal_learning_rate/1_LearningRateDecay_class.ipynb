{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### LearningRateDecay"
      ],
      "metadata": {
        "id": "z2_BsgRUbPdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the necessary packages\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "class LearningRateDecay:\n",
        "  pass\n",
        "\n",
        "class StepDecay:\n",
        "  def __init__(self, initAlpha=0.01, factor = 0.25, dropEvery=10):\n",
        "    self.initAlpha = initAlpha\n",
        "    self.factor = factor\n",
        "    self.dropEvery = dropEvery\n",
        "\n",
        "\n",
        "  def plot(self, epochs:list, title=\"Learning Rate Schedule\"):\n",
        "    lrs = [self(i) for i in epochs]\n",
        "\n",
        "    plt.style.use('ggplot')\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, lrs)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Epoch #')\n",
        "    plt.ylabel('Learning Rate')\n",
        "\n",
        "  def __call__(self, epoch):\n",
        "    # compute the learning rate for the current epoch\n",
        "    exp = np.floor((1 + epoch) / self.dropEvery)\n",
        "    alpha = self.initAlpha * (self.factor ** exp)\n",
        "    # return the learning rate\n",
        "    return float(alpha)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class PolynomialDecay(LearningRateDecay):\n",
        "  def __init__(self, maxEpochs=100, initAlpha=0.01, power=1.0):       # Note that if you set power=1.0 then you have a linear learning rate decay.\n",
        "    # store the maximum number of epochs, base learning rate, and power of the polynomial\n",
        "    self.maxEpochs = maxEpochs\n",
        "    self.initAlpha = initAlpha\n",
        "    self.power = power\n",
        "\n",
        "  def __call__(self, epoch):\n",
        "    # compute the new learning rate based on polynomial decay\n",
        "    decay = (1 - (epoch / float(self.maxEpochs))) ** self.power\n",
        "    alpha = self.initAlpha * decay\n",
        "    # return the new learning rate\n",
        "    return float(alpha)\n",
        "\n"
      ],
      "metadata": {
        "id": "1GQhg_NAbFPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set the matplotlib backend so figures can be saved in the background\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "# import the necessary packages\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "epochs = 20\n",
        "callbacks = []\n",
        "schedule = StepDecay(initAlpha=1e-1, factor=0.25, dropEvery=15)\n",
        "callbacks = [LearningRateScheduler(schedule)]\n",
        "\n",
        "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
        "trainX = trainX.astype(\"float\") / 255.0\n",
        "testX = testX.astype(\"float\") / 255.0\n",
        "\n",
        "# convert the labels from integers to vectors\n",
        "lb = LabelBinarizer()\n",
        "trainY = lb.fit_transform(trainY)\n",
        "testY = lb.transform(testY)\n",
        "\n",
        "labelNames = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
        "\n",
        "decay = 1e-1 / epochs\n",
        "\n",
        "# initialize our optimizer and model, then compile it\n",
        "opt =Adam(\n",
        "    learning_rate=0.001,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-07,\n",
        "    amsgrad=False,\n",
        "    weight_decay= decay)\n",
        "\n",
        "model = ResNet50(\n",
        "    include_top=True,\n",
        "    weights=None,\n",
        "    input_tensor=None,\n",
        "    input_shape=(32,32,3),\n",
        "    pooling=\"max\",\n",
        "    classes=10,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "\n",
        "# train the network\n",
        "H = model.fit(x=trainX, y=trainY, validation_data=(testX, testY),\n",
        "\tbatch_size=128, epochs=epochs, callbacks=callbacks, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BbZQzSYWBJE",
        "outputId": "cd6b8f46-257b-4cb2-f1cc-b9506a644d05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "391/391 [==============================] - 73s 72ms/step - loss: 3.4808 - accuracy: 0.1019 - val_loss: 2.3100 - val_accuracy: 0.1000 - lr: 0.1000\n",
            "Epoch 2/20\n",
            "391/391 [==============================] - 26s 67ms/step - loss: 2.2883 - accuracy: 0.1159 - val_loss: 3.5197 - val_accuracy: 0.0673 - lr: 0.1000\n",
            "Epoch 3/20\n",
            "391/391 [==============================] - 27s 70ms/step - loss: 1.9514 - accuracy: 0.2312 - val_loss: 2.0507 - val_accuracy: 0.2000 - lr: 0.1000\n",
            "Epoch 4/20\n",
            "391/391 [==============================] - 26s 67ms/step - loss: 1.8664 - accuracy: 0.2609 - val_loss: 2.3792 - val_accuracy: 0.1649 - lr: 0.1000\n",
            "Epoch 5/20\n",
            "391/391 [==============================] - 27s 68ms/step - loss: 1.8590 - accuracy: 0.2635 - val_loss: 2.6565 - val_accuracy: 0.1796 - lr: 0.1000\n",
            "Epoch 6/20\n",
            "391/391 [==============================] - 27s 68ms/step - loss: 2.2048 - accuracy: 0.2364 - val_loss: 1188.0146 - val_accuracy: 0.1009 - lr: 0.1000\n",
            "Epoch 7/20\n",
            "391/391 [==============================] - 28s 73ms/step - loss: 1.9041 - accuracy: 0.2468 - val_loss: 3.3806 - val_accuracy: 0.1216 - lr: 0.1000\n",
            "Epoch 8/20\n",
            "391/391 [==============================] - 27s 68ms/step - loss: 1.8624 - accuracy: 0.2603 - val_loss: 3.4730 - val_accuracy: 0.1672 - lr: 0.1000\n",
            "Epoch 9/20\n",
            "391/391 [==============================] - 27s 69ms/step - loss: 1.8683 - accuracy: 0.2613 - val_loss: 3.6128 - val_accuracy: 0.1174 - lr: 0.1000\n",
            "Epoch 10/20\n",
            "391/391 [==============================] - 27s 68ms/step - loss: 1.7759 - accuracy: 0.2981 - val_loss: 7.1876 - val_accuracy: 0.1441 - lr: 0.1000\n",
            "Epoch 11/20\n",
            "391/391 [==============================] - 27s 68ms/step - loss: 1.6448 - accuracy: 0.3654 - val_loss: 2.3493 - val_accuracy: 0.2140 - lr: 0.1000\n",
            "Epoch 12/20\n",
            "391/391 [==============================] - 27s 68ms/step - loss: 1.5696 - accuracy: 0.3966 - val_loss: 1.9530 - val_accuracy: 0.2980 - lr: 0.1000\n",
            "Epoch 13/20\n",
            "391/391 [==============================] - 27s 69ms/step - loss: 1.5104 - accuracy: 0.4303 - val_loss: 26.3265 - val_accuracy: 0.1566 - lr: 0.1000\n",
            "Epoch 14/20\n",
            "391/391 [==============================] - 26s 68ms/step - loss: 1.4025 - accuracy: 0.4760 - val_loss: 21.1535 - val_accuracy: 0.1823 - lr: 0.1000\n",
            "Epoch 15/20\n",
            "391/391 [==============================] - 27s 68ms/step - loss: 1.1497 - accuracy: 0.5700 - val_loss: 1.6142 - val_accuracy: 0.4274 - lr: 0.0250\n",
            "Epoch 16/20\n",
            "391/391 [==============================] - 27s 69ms/step - loss: 1.0626 - accuracy: 0.6207 - val_loss: 1.3952 - val_accuracy: 0.5203 - lr: 0.0250\n",
            "Epoch 17/20\n",
            "391/391 [==============================] - 26s 68ms/step - loss: 0.9964 - accuracy: 0.6534 - val_loss: 1.2231 - val_accuracy: 0.5668 - lr: 0.0250\n",
            "Epoch 18/20\n",
            "391/391 [==============================] - 27s 68ms/step - loss: 0.9459 - accuracy: 0.6722 - val_loss: 1.2893 - val_accuracy: 0.5626 - lr: 0.0250\n",
            "Epoch 19/20\n",
            "391/391 [==============================] - 27s 68ms/step - loss: 0.9130 - accuracy: 0.6859 - val_loss: 2.2548 - val_accuracy: 0.4527 - lr: 0.0250\n",
            "Epoch 20/20\n",
            "391/391 [==============================] - 26s 68ms/step - loss: 0.8592 - accuracy: 0.7055 - val_loss: 1.3491 - val_accuracy: 0.5429 - lr: 0.0250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the network\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predictions = model.predict(x=testX, batch_size=128)\n",
        "print(classification_report(testY.argmax(axis=1),\n",
        "\tpredictions.argmax(axis=1), target_names=labelNames))\n",
        "# plot the training loss and accuracy\n",
        "N = np.arange(0, epochs)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(N, H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(N, H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy on CIFAR-10\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend()\n",
        "plt.savefig(\"train_plot\")\n",
        "# if the learning rate schedule is not empty, then save the learning\n",
        "# rate plot\n",
        "if schedule is not None:\n",
        "\tschedule.plot(N)\n",
        "\tplt.savefig(\"lr_plot\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nexto35ser-Q",
        "outputId": "b355aece-233e-4ed5-d753-1b5d596b29bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] evaluating network...\n",
            "79/79 [==============================] - 3s 14ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    airplane       0.75      0.63      0.68      1000\n",
            "  automobile       0.92      0.49      0.64      1000\n",
            "        bird       0.89      0.12      0.21      1000\n",
            "         cat       0.23      0.86      0.37      1000\n",
            "        deer       0.67      0.49      0.57      1000\n",
            "         dog       0.77      0.13      0.22      1000\n",
            "        frog       0.92      0.43      0.59      1000\n",
            "       horse       0.78      0.64      0.70      1000\n",
            "        ship       0.59      0.91      0.71      1000\n",
            "       truck       0.69      0.72      0.70      1000\n",
            "\n",
            "    accuracy                           0.54     10000\n",
            "   macro avg       0.72      0.54      0.54     10000\n",
            "weighted avg       0.72      0.54      0.54     10000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-b36c3eb99d05>:19: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
            "  plt.figure()\n"
          ]
        }
      ]
    }
  ]
}