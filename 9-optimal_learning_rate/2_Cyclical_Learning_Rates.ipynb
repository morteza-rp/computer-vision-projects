{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Cyclical Learning Rates\n",
        "\n",
        "Using Cyclical Learning Rates you can dramatically reduce the number of experiments required to tune and find an optimal learning rate for your model.\n",
        "\n",
        "there are two problems with basic learning rate schedules:\n",
        "\n",
        "1-We dont know what the optimal initial learning rate is.\n",
        "\n",
        "2-Monotonically decreasing our learning rate may lead to our network getting “stuck” in plateaus of the loss landscape."
      ],
      "metadata": {
        "id": "giyI6phRRYMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/bckenstler/CLR.git"
      ],
      "metadata": {
        "id": "Qfv_jlb4zjk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mx_CquxWRTze"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# initialize the list of class label names\n",
        "CLASSES = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
        "\n",
        "# define the minimum learning rate, maximum learning rate, batch size, step size, CLR method, and number of epochs\n",
        "MIN_LR = 1e-7\n",
        "MAX_LR = 1e-2\n",
        "BATCH_SIZE = 64\n",
        "STEP_SIZE = 8\n",
        "CLR_METHOD = \"triangular\"\n",
        "NUM_EPOCHS = 96\n",
        "\n",
        "# define the path to the output training history plot and cyclical learning rate plot\n",
        "TRAINING_PLOT_PATH = os.path.sep.join([\"output\", \"training_plot.png\"])\n",
        "CLR_PLOT_PATH = os.path.sep.join([\"output\", \"clr_plot.png\"])\n",
        "\n",
        "\n",
        "# set the matplotlib backend so figures can be saved in the background\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "# import the necessary packages\n",
        "from clr_callback import CyclicLR\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers.legacy import SGD\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications.mobilenet import MobileNet\n",
        "\n",
        "\n",
        "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
        "trainX = trainX.astype(\"float\")\n",
        "testX = testX.astype(\"float\")\n",
        "\n",
        "# apply mean subtraction to the data\n",
        "mean = np.mean(trainX, axis=0)\n",
        "trainX -= mean\n",
        "testX -= mean\n",
        "# convert the labels from integers to vectors\n",
        "lb = LabelBinarizer()\n",
        "trainY = lb.fit_transform(trainY)\n",
        "testY = lb.transform(testY)\n",
        "\n",
        "\n",
        "aug = ImageDataGenerator(width_shift_range=0.1,\n",
        "\theight_shift_range=0.1, horizontal_flip=True,\n",
        "\tfill_mode=\"nearest\")\n",
        "\n",
        "\n",
        "model = MobileNet(\n",
        "    input_shape=(32, 32, 3),\n",
        "    alpha=1.0,\n",
        "    depth_multiplier=1,\n",
        "    dropout=0.001,\n",
        "    include_top=True,\n",
        "    weights=None,\n",
        "    input_tensor=None,\n",
        "    pooling=None,\n",
        "    classes=10,\n",
        "    classifier_activation='softmax')\n",
        "\n",
        "\n",
        "opt = SGD(learning_rate=0.001)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
        "\n",
        "clr = CyclicLR(\n",
        "\tmode= CLR_METHOD,\n",
        "\tbase_lr= MIN_LR,\n",
        "\tmax_lr= MAX_LR,\n",
        "\tstep_size=  STEP_SIZE * (trainX.shape[0] //  BATCH_SIZE))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "H = model.fit(\n",
        "  x= aug.flow(trainX, trainY, batch_size= BATCH_SIZE),\n",
        "  validation_data=(testX, testY),\n",
        "  steps_per_epoch=trainX.shape[0] // BATCH_SIZE,\n",
        "  epochs=NUM_EPOCHS,\n",
        "  callbacks=[clr],\n",
        "  verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcny2f4eWx0P",
        "outputId": "188cdf07-3edd-4a3c-f16c-653e3dff6744"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/96\n",
            "781/781 [==============================] - 47s 56ms/step - loss: 2.5051 - accuracy: 0.1130 - val_loss: 2.4062 - val_accuracy: 0.1240\n",
            "Epoch 2/96\n",
            "781/781 [==============================] - 34s 44ms/step - loss: 2.2144 - accuracy: 0.1811 - val_loss: 2.2053 - val_accuracy: 0.2049\n",
            "Epoch 3/96\n",
            "781/781 [==============================] - 37s 47ms/step - loss: 1.9570 - accuracy: 0.2622 - val_loss: 1.9537 - val_accuracy: 0.2915\n",
            "Epoch 4/96\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 1.8106 - accuracy: 0.3169 - val_loss: 1.7444 - val_accuracy: 0.3314\n",
            "Epoch 5/96\n",
            "781/781 [==============================] - 34s 44ms/step - loss: 1.6899 - accuracy: 0.3682 - val_loss: 1.6828 - val_accuracy: 0.3813\n",
            "Epoch 6/96\n",
            "781/781 [==============================] - 35s 44ms/step - loss: 1.6058 - accuracy: 0.4061 - val_loss: 1.5757 - val_accuracy: 0.4284\n",
            "Epoch 7/96\n",
            "781/781 [==============================] - 35s 44ms/step - loss: 1.5308 - accuracy: 0.4364 - val_loss: 1.5371 - val_accuracy: 0.4403\n",
            "Epoch 8/96\n",
            "781/781 [==============================] - 35s 44ms/step - loss: 1.4820 - accuracy: 0.4568 - val_loss: 1.4428 - val_accuracy: 0.4765\n",
            "Epoch 9/96\n",
            "781/781 [==============================] - 36s 46ms/step - loss: 1.4169 - accuracy: 0.4823 - val_loss: 1.3289 - val_accuracy: 0.5138\n",
            "Epoch 10/96\n",
            "781/781 [==============================] - 36s 46ms/step - loss: 1.3555 - accuracy: 0.5095 - val_loss: 1.2958 - val_accuracy: 0.5318\n",
            "Epoch 11/96\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 1.3055 - accuracy: 0.5242 - val_loss: 1.2378 - val_accuracy: 0.5497\n",
            "Epoch 12/96\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 1.2631 - accuracy: 0.5453 - val_loss: 1.1871 - val_accuracy: 0.5694\n",
            "Epoch 13/96\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 1.2277 - accuracy: 0.5561 - val_loss: 1.1525 - val_accuracy: 0.5806\n",
            "Epoch 14/96\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 1.1955 - accuracy: 0.5675 - val_loss: 1.1531 - val_accuracy: 0.5857\n",
            "Epoch 15/96\n",
            "781/781 [==============================] - 36s 46ms/step - loss: 1.1777 - accuracy: 0.5770 - val_loss: 1.1254 - val_accuracy: 0.5899\n",
            "Epoch 16/96\n",
            "781/781 [==============================] - 35s 44ms/step - loss: 1.1551 - accuracy: 0.5844 - val_loss: 1.1181 - val_accuracy: 0.5951\n",
            "Epoch 17/96\n",
            "781/781 [==============================] - 35s 44ms/step - loss: 1.1556 - accuracy: 0.5847 - val_loss: 1.1128 - val_accuracy: 0.5964\n",
            "Epoch 18/96\n",
            "781/781 [==============================] - 34s 44ms/step - loss: 1.1520 - accuracy: 0.5845 - val_loss: 1.1128 - val_accuracy: 0.6005\n",
            "Epoch 19/96\n",
            "781/781 [==============================] - 37s 48ms/step - loss: 1.1525 - accuracy: 0.5860 - val_loss: 1.1167 - val_accuracy: 0.5997\n",
            "Epoch 20/96\n",
            "781/781 [==============================] - 36s 47ms/step - loss: 1.1486 - accuracy: 0.5854 - val_loss: 1.1291 - val_accuracy: 0.5956\n",
            "Epoch 21/96\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 1.1451 - accuracy: 0.5890 - val_loss: 1.1362 - val_accuracy: 0.5936\n",
            "Epoch 22/96\n",
            "781/781 [==============================] - 36s 46ms/step - loss: 1.1394 - accuracy: 0.5910 - val_loss: 1.1152 - val_accuracy: 0.6000\n",
            "Epoch 23/96\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 1.1358 - accuracy: 0.5946 - val_loss: 1.0906 - val_accuracy: 0.6113\n",
            "Epoch 24/96\n",
            "781/781 [==============================] - 34s 44ms/step - loss: 1.1240 - accuracy: 0.5964 - val_loss: 1.1077 - val_accuracy: 0.6060\n",
            "Epoch 25/96\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 1.1029 - accuracy: 0.6071 - val_loss: 1.0768 - val_accuracy: 0.6149\n",
            "Epoch 26/96\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 1.0632 - accuracy: 0.6218 - val_loss: 1.0613 - val_accuracy: 0.6234\n",
            "Epoch 27/96\n",
            "781/781 [==============================] - 35s 44ms/step - loss: 1.0312 - accuracy: 0.6314 - val_loss: 0.9865 - val_accuracy: 0.6502\n",
            "Epoch 28/96\n",
            "781/781 [==============================] - 35s 44ms/step - loss: 1.0042 - accuracy: 0.6419 - val_loss: 0.9767 - val_accuracy: 0.6523\n",
            "Epoch 29/96\n",
            "781/781 [==============================] - 35s 44ms/step - loss: 0.9806 - accuracy: 0.6508 - val_loss: 0.9891 - val_accuracy: 0.6512\n",
            "Epoch 30/96\n",
            "781/781 [==============================] - 35s 44ms/step - loss: 0.9609 - accuracy: 0.6580 - val_loss: 0.9638 - val_accuracy: 0.6542\n",
            "Epoch 31/96\n",
            "781/781 [==============================] - 34s 44ms/step - loss: 0.9382 - accuracy: 0.6660 - val_loss: 0.9420 - val_accuracy: 0.6678\n",
            "Epoch 32/96\n",
            "781/781 [==============================] - 35s 44ms/step - loss: 0.9310 - accuracy: 0.6678 - val_loss: 0.9334 - val_accuracy: 0.6695\n",
            "Epoch 33/96\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 0.9236 - accuracy: 0.6705 - val_loss: 0.9361 - val_accuracy: 0.6687\n",
            "Epoch 34/96\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.9265 - accuracy: 0.6708 - val_loss: 0.9412 - val_accuracy: 0.6706\n",
            "Epoch 35/96\n",
            "781/781 [==============================] - 36s 47ms/step - loss: 0.9255 - accuracy: 0.6686 - val_loss: 0.9516 - val_accuracy: 0.6610\n",
            "Epoch 36/96\n",
            "781/781 [==============================] - 34s 44ms/step - loss: 0.9312 - accuracy: 0.6669 - val_loss: 0.9340 - val_accuracy: 0.6681\n",
            "Epoch 37/96\n",
            "781/781 [==============================] - 35s 44ms/step - loss: 0.9390 - accuracy: 0.6652 - val_loss: 0.9472 - val_accuracy: 0.6706\n",
            "Epoch 38/96\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 0.9395 - accuracy: 0.6660 - val_loss: 0.9494 - val_accuracy: 0.6609\n",
            "Epoch 39/96\n",
            "781/781 [==============================] - 35s 44ms/step - loss: 0.9385 - accuracy: 0.6654 - val_loss: 0.9240 - val_accuracy: 0.6741\n",
            "Epoch 40/96\n",
            "781/781 [==============================] - 37s 47ms/step - loss: 0.9358 - accuracy: 0.6668 - val_loss: 1.0262 - val_accuracy: 0.6381\n",
            "Epoch 41/96\n",
            "781/781 [==============================] - 36s 46ms/step - loss: 0.9231 - accuracy: 0.6719 - val_loss: 0.9523 - val_accuracy: 0.6656\n",
            "Epoch 42/96\n",
            "781/781 [==============================] - 35s 44ms/step - loss: 0.8974 - accuracy: 0.6814 - val_loss: 0.9632 - val_accuracy: 0.6632\n",
            "Epoch 43/96\n",
            "781/781 [==============================] - 36s 46ms/step - loss: 0.8738 - accuracy: 0.6909 - val_loss: 0.8909 - val_accuracy: 0.6864\n",
            "Epoch 44/96\n",
            "781/781 [==============================] - 34s 44ms/step - loss: 0.8462 - accuracy: 0.7004 - val_loss: 0.8610 - val_accuracy: 0.7003\n",
            "Epoch 45/96\n",
            "781/781 [==============================] - 34s 44ms/step - loss: 0.8367 - accuracy: 0.7043 - val_loss: 0.8487 - val_accuracy: 0.7010\n",
            "Epoch 46/96\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 0.8155 - accuracy: 0.7099 - val_loss: 0.8387 - val_accuracy: 0.7064\n",
            "Epoch 47/96\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 0.8011 - accuracy: 0.7159 - val_loss: 0.8269 - val_accuracy: 0.7090\n",
            "Epoch 48/96\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.7841 - accuracy: 0.7230 - val_loss: 0.8271 - val_accuracy: 0.7083\n",
            "Epoch 49/96\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 0.7846 - accuracy: 0.7212 - val_loss: 0.8238 - val_accuracy: 0.7115\n",
            "Epoch 50/96\n",
            "781/781 [==============================] - 35s 44ms/step - loss: 0.7836 - accuracy: 0.7225 - val_loss: 0.8407 - val_accuracy: 0.7022\n",
            "Epoch 51/96\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 0.7887 - accuracy: 0.7195 - val_loss: 0.8445 - val_accuracy: 0.7017\n",
            "Epoch 52/96\n",
            "781/781 [==============================] - 35s 44ms/step - loss: 0.7940 - accuracy: 0.7214 - val_loss: 0.8561 - val_accuracy: 0.7059\n",
            "Epoch 53/96\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.7995 - accuracy: 0.7153 - val_loss: 0.8244 - val_accuracy: 0.7074\n",
            "Epoch 54/96\n",
            "781/781 [==============================] - 35s 44ms/step - loss: 0.8035 - accuracy: 0.7156 - val_loss: 0.8740 - val_accuracy: 0.6949\n",
            "Epoch 55/96\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 0.8094 - accuracy: 0.7125 - val_loss: 0.8705 - val_accuracy: 0.6927\n",
            "Epoch 56/96\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 0.8084 - accuracy: 0.7111 - val_loss: 0.8840 - val_accuracy: 0.6958\n",
            "Epoch 57/96\n",
            "781/781 [==============================] - 37s 47ms/step - loss: 0.8036 - accuracy: 0.7152 - val_loss: 0.8421 - val_accuracy: 0.7049\n",
            "Epoch 58/96\n",
            "781/781 [==============================] - 36s 46ms/step - loss: 0.7832 - accuracy: 0.7219 - val_loss: 0.9103 - val_accuracy: 0.6868\n",
            "Epoch 59/96\n",
            "781/781 [==============================] - 35s 44ms/step - loss: 0.7718 - accuracy: 0.7264 - val_loss: 0.8257 - val_accuracy: 0.7111\n",
            "Epoch 60/96\n",
            "781/781 [==============================] - 36s 46ms/step - loss: 0.7424 - accuracy: 0.7362 - val_loss: 0.8091 - val_accuracy: 0.7169\n",
            "Epoch 61/96\n",
            "781/781 [==============================] - 36s 46ms/step - loss: 0.7265 - accuracy: 0.7412 - val_loss: 0.7890 - val_accuracy: 0.7245\n",
            "Epoch 62/96\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 0.7079 - accuracy: 0.7489 - val_loss: 0.7792 - val_accuracy: 0.7288\n",
            "Epoch 63/96\n",
            "781/781 [==============================] - 35s 44ms/step - loss: 0.6955 - accuracy: 0.7553 - val_loss: 0.7663 - val_accuracy: 0.7327\n",
            "Epoch 64/96\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.6804 - accuracy: 0.7598 - val_loss: 0.7665 - val_accuracy: 0.7306\n",
            "Epoch 65/96\n",
            "781/781 [==============================] - 36s 45ms/step - loss: 0.6790 - accuracy: 0.7585 - val_loss: 0.7673 - val_accuracy: 0.7300\n",
            "Epoch 66/96\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 0.6870 - accuracy: 0.7562 - val_loss: 0.7717 - val_accuracy: 0.7277\n",
            "Epoch 67/96\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 0.6914 - accuracy: 0.7542 - val_loss: 0.7714 - val_accuracy: 0.7316\n",
            "Epoch 68/96\n",
            "781/781 [==============================] - 35s 44ms/step - loss: 0.6906 - accuracy: 0.7560 - val_loss: 0.8100 - val_accuracy: 0.7172\n",
            "Epoch 69/96\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 0.7019 - accuracy: 0.7509 - val_loss: 0.8159 - val_accuracy: 0.7139\n",
            "Epoch 70/96\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 0.7094 - accuracy: 0.7493 - val_loss: 0.7858 - val_accuracy: 0.7259\n",
            "Epoch 71/96\n",
            "781/781 [==============================] - 36s 46ms/step - loss: 0.7164 - accuracy: 0.7469 - val_loss: 0.8320 - val_accuracy: 0.7068\n",
            "Epoch 72/96\n",
            "781/781 [==============================] - 36s 46ms/step - loss: 0.7232 - accuracy: 0.7419 - val_loss: 0.8255 - val_accuracy: 0.7126\n",
            "Epoch 73/96\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 0.7224 - accuracy: 0.7443 - val_loss: 0.8028 - val_accuracy: 0.7199\n",
            "Epoch 74/96\n",
            "781/781 [==============================] - 36s 46ms/step - loss: 0.7004 - accuracy: 0.7528 - val_loss: 0.8012 - val_accuracy: 0.7231\n",
            "Epoch 75/96\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.6820 - accuracy: 0.7565 - val_loss: 0.7711 - val_accuracy: 0.7332\n",
            "Epoch 76/96\n",
            "781/781 [==============================] - 36s 46ms/step - loss: 0.6665 - accuracy: 0.7647 - val_loss: 0.7460 - val_accuracy: 0.7422\n",
            "Epoch 77/96\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 0.6476 - accuracy: 0.7713 - val_loss: 0.7402 - val_accuracy: 0.7411\n",
            "Epoch 78/96\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 0.6306 - accuracy: 0.7751 - val_loss: 0.7416 - val_accuracy: 0.7430\n",
            "Epoch 79/96\n",
            "781/781 [==============================] - 36s 47ms/step - loss: 0.6185 - accuracy: 0.7816 - val_loss: 0.7398 - val_accuracy: 0.7419\n",
            "Epoch 80/96\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 0.6090 - accuracy: 0.7835 - val_loss: 0.7306 - val_accuracy: 0.7444\n",
            "Epoch 81/96\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 0.6028 - accuracy: 0.7858 - val_loss: 0.7313 - val_accuracy: 0.7451\n",
            "Epoch 82/96\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.6070 - accuracy: 0.7827 - val_loss: 0.7308 - val_accuracy: 0.7473\n",
            "Epoch 83/96\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 0.6091 - accuracy: 0.7826 - val_loss: 0.7379 - val_accuracy: 0.7478\n",
            "Epoch 84/96\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 0.6208 - accuracy: 0.7790 - val_loss: 0.7493 - val_accuracy: 0.7422\n",
            "Epoch 85/96\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 0.6246 - accuracy: 0.7784 - val_loss: 0.7466 - val_accuracy: 0.7423\n",
            "Epoch 86/96\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 0.6333 - accuracy: 0.7747 - val_loss: 0.7672 - val_accuracy: 0.7355\n",
            "Epoch 87/96\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 0.6407 - accuracy: 0.7708 - val_loss: 0.7752 - val_accuracy: 0.7322\n",
            "Epoch 88/96\n",
            "781/781 [==============================] - 36s 46ms/step - loss: 0.6539 - accuracy: 0.7664 - val_loss: 0.7794 - val_accuracy: 0.7337\n",
            "Epoch 89/96\n",
            "781/781 [==============================] - 35s 44ms/step - loss: 0.6519 - accuracy: 0.7686 - val_loss: 0.7601 - val_accuracy: 0.7395\n",
            "Epoch 90/96\n",
            "781/781 [==============================] - 36s 46ms/step - loss: 0.6321 - accuracy: 0.7761 - val_loss: 0.7383 - val_accuracy: 0.7416\n",
            "Epoch 91/96\n",
            "781/781 [==============================] - 36s 46ms/step - loss: 0.6145 - accuracy: 0.7822 - val_loss: 0.7629 - val_accuracy: 0.7373\n",
            "Epoch 92/96\n",
            "781/781 [==============================] - 35s 44ms/step - loss: 0.5947 - accuracy: 0.7884 - val_loss: 0.7287 - val_accuracy: 0.7467\n",
            "Epoch 93/96\n",
            "781/781 [==============================] - 36s 46ms/step - loss: 0.5754 - accuracy: 0.7963 - val_loss: 0.7130 - val_accuracy: 0.7536\n",
            "Epoch 94/96\n",
            "781/781 [==============================] - 36s 46ms/step - loss: 0.5612 - accuracy: 0.7996 - val_loss: 0.7109 - val_accuracy: 0.7561\n",
            "Epoch 95/96\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 0.5501 - accuracy: 0.8022 - val_loss: 0.7034 - val_accuracy: 0.7593\n",
            "Epoch 96/96\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 0.5347 - accuracy: 0.8095 - val_loss: 0.7048 - val_accuracy: 0.7591\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(x=testX, batch_size=BATCH_SIZE)\n",
        "print(classification_report(testY.argmax(axis=1),\n",
        "\tpredictions.argmax(axis=1), target_names=CLASSES))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnwNc6IsjNpF",
        "outputId": "c77c03fb-5f2c-45b5-f5fe-b9e21e515737"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "157/157 [==============================] - 1s 4ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    airplane       0.78      0.81      0.79      1000\n",
            "  automobile       0.84      0.90      0.87      1000\n",
            "        bird       0.77      0.62      0.69      1000\n",
            "         cat       0.61      0.52      0.56      1000\n",
            "        deer       0.73      0.69      0.71      1000\n",
            "         dog       0.67      0.64      0.65      1000\n",
            "        frog       0.69      0.89      0.78      1000\n",
            "       horse       0.78      0.82      0.80      1000\n",
            "        ship       0.90      0.84      0.87      1000\n",
            "       truck       0.82      0.86      0.84      1000\n",
            "\n",
            "    accuracy                           0.76     10000\n",
            "   macro avg       0.76      0.76      0.76     10000\n",
            "weighted avg       0.76      0.76      0.76     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the training loss and accuracy\n",
        "N = np.arange(0, NUM_EPOCHS)\n",
        "# plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(N, H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(N, H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig('training_plot.png')\n",
        "\n",
        "# plot the learning rate history\n",
        "N = np.arange(0, len(clr.history[\"lr\"]))\n",
        "plt.figure()\n",
        "plt.plot(N, clr.history[\"lr\"])\n",
        "plt.title(\"Cyclical Learning Rate (CLR)\")\n",
        "plt.xlabel(\"Training Iterations\")\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.savefig(\"Learning Rate\")"
      ],
      "metadata": {
        "id": "ADvPQ4oG2EYK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yIUVts7w3Hln"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}